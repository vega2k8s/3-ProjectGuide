# Day 5: 컨테이너화 및 CI/CD (최신 LangServe 통합)

## 산출물 1: 최신 Docker 설정 파일들

### AI Module Dockerfile (LangServe 최적화)
```dockerfile
# ai-module/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# 시스템 의존성 설치
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Python 의존성 복사 및 설치
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# 소스 코드 복사
COPY src/ ./src/
COPY data/ ./data/
COPY .env ./

# 필요한 디렉토리 생성
RUN mkdir -p data/vector_store uploaded_documents logs

# 애플리케이션 사용자 생성
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# 포트 노출 (LangServe 기본 포트)
EXPOSE 8000

# 환경 변수 설정
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV LANGCHAIN_TRACING_V2=true
ENV LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
ENV LANGCHAIN_PROJECT=smart-learning-production

# 헬스체크 (LangServe 엔드포인트)
HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# LangServe 애플리케이션 실행
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

### Backend Dockerfile (AI 통합 최적화)
```dockerfile
# backend/Dockerfile
FROM maven:3.9-openjdk-17-slim AS build

WORKDIR /app

# Maven 설정 및 의존성 복사
COPY .mvn .mvn
COPY mvnw pom.xml ./

# 의존성 다운로드 (캐시 최적화)
RUN ./mvnw dependency:go-offline -B

# 소스 코드 복사 및 빌드
COPY src ./src
RUN ./mvnw clean package -DskipTests -B

# 실행 스테이지
FROM openjdk:17-jdk-slim

WORKDIR /app

# 시스템 패키지 업데이트
RUN apt-get update && apt-get install -y \
    curl \
    netcat-traditional \
    && rm -rf /var/lib/apt/lists/*

# JAR 파일 복사
COPY --from=build /app/target/*.jar app.jar

# 업로드 디렉토리 생성
RUN mkdir -p uploads logs

# 애플리케이션 사용자 생성
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# 포트 노출
EXPOSE 8080

# JVM 최적화 설정
ENV JAVA_OPTS="-Xmx1g -Xms512m -XX:+UseG1GC -XX:+UseContainerSupport"

# 헬스체크
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

# 대기 스크립트 생성
RUN echo '#!/bin/bash\n\
echo "Waiting for AI service to be ready..."\n\
while ! nc -z ai-service 8000; do\n\
  sleep 2\n\
done\n\
echo "AI service is ready!"\n\
exec java $JAVA_OPTS -jar app.jar' > wait-and-start.sh && \
chmod +x wait-and-start.sh

# 애플리케이션 실행
ENTRYPOINT ["./wait-and-start.sh"]
```

### Frontend Dockerfile (최신 React + AI 통합)
```dockerfile
# frontend/Dockerfile
# Build stage
FROM node:18-alpine AS build

WORKDIR /app

# package.json과 lock 파일 복사 (캐시 최적화)
COPY package.json package-lock.json ./

# 의존성 설치
RUN npm ci --only=production --silent

# 소스 코드 복사 및 빌드
COPY . .

# 환경별 빌드 설정
ARG REACT_APP_API_URL
ARG REACT_APP_AI_URL
ARG NODE_ENV=production

ENV REACT_APP_API_URL=$REACT_APP_API_URL
ENV REACT_APP_AI_URL=$REACT_APP_AI_URL
ENV NODE_ENV=$NODE_ENV

RUN npm run build

# Production stage
FROM nginx:alpine

# 보안 업데이트
RUN apk update && apk upgrade && apk add --no-cache curl

# Nginx 설정 복사
COPY nginx.conf /etc/nginx/nginx.conf

# 빌드된 파일 복사
COPY --from=build /app/build /usr/share/nginx/html

# 권한 설정
RUN chown -R nginx:nginx /usr/share/nginx/html && \
    chmod -R 755 /usr/share/nginx/html

# 포트 노출
EXPOSE 80

# 헬스체크
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
    CMD curl -f http://localhost/health || exit 1

# Nginx 실행
CMD ["nginx", "-g", "daemon off;"]
```

### 최신 Nginx 설정 (LangServe 라우팅 포함)
```nginx
# frontend/nginx.conf
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    
    # 로깅 설정
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    # 업스트림 정의
    upstream backend {
        server backend:8080 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    upstream ai-service {
        server ai-service:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    # 성능 최적화
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50m;
    
    # 버퍼 크기 최적화
    proxy_buffer_size 128k;
    proxy_buffers 4 256k;
    proxy_busy_buffers_size 256k;
    
    # GZIP 압축
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json
        application/atom+xml
        image/svg+xml;
    
    server {
        listen 80;
        server_name localhost;
        root /usr/share/nginx/html;
        index index.html;
        
        # 헬스체크 엔드포인트
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # SPA 라우팅 지원
        location / {
            try_files $uri $uri/ /index.html;
            
            # 캐시 설정
            add_header Cache-Control "no-cache, no-store, must-revalidate";
            add_header Pragma "no-cache";
            add_header Expires "0";
        }
        
        # 정적 파일 캐싱
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
            add_header X-Content-Type-Options "nosniff";
        }
        
        # Backend API 프록시
        location /api/ {
            proxy_pass http://backend/api/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
            
            # 타임아웃 설정
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # AI 서비스 프록시 (LangServe)
        location /ai/ {
            rewrite ^/ai/(.*) /$1 break;
            proxy_pass http://ai-service;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # 스트리밍 최적화
            proxy_buffering off;
            proxy_cache off;
            proxy_read_timeout 300s;
            proxy_connect_timeout 75s;
            
            # Server-Sent Events 지원
            proxy_set_header Connection '';
            chunked_transfer_encoding off;
        }
        
        # LangServe Playground 접근
        location /langserve/ {
            proxy_pass http://ai-service/;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Playground 최적화
            proxy_buffering off;
            proxy_cache off;
        }
        
        # WebSocket 지원
        location /ws/ {
            proxy_pass http://backend/ws/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # 메트릭 프록시
        location /metrics {
            proxy_pass http://backend/actuator/metrics;
            proxy_set_header Host $host;
            access_log off;
        }
        
        # 보안 헤더
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;
        add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; connect-src 'self' ws: wss:;" always;
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    }
}
```

## 산출물 2: 최신 Docker Compose 설정

### docker-compose.yml (LangServe 통합)
```yaml
version: '3.8'

services:
  # 데이터베이스
  database:
    image: mariadb:10.11
    container_name: smart-learning-db
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD:-rootpassword}
      MYSQL_DATABASE: ${DB_NAME:-smart_learning_db}
      MYSQL_USER: ${DB_USER:-app_user}
      MYSQL_PASSWORD: ${DB_PASSWORD:-app_password}
      MYSQL_CHARSET: utf8mb4
      MYSQL_COLLATION: utf8mb4_unicode_ci
    ports:
      - "${DB_PORT:-3306}:3306"
    volumes:
      - db_data:/var/lib/mysql
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./database/my.cnf:/etc/mysql/conf.d/my.cnf:ro
    networks:
      - app-network
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${DB_ROOT_PASSWORD:-rootpassword}"]
      timeout: 20s
      retries: 10
      interval: 30s
      start_period: 40s

  # Redis 캐시
  redis:
    image: redis:7-alpine
    container_name: smart-learning-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_password}
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      timeout: 3s
      retries: 5
      interval: 30s

  # AI 서비스 (LangServe)
  ai-service:
    build:
      context: ./ai-module
      dockerfile: Dockerfile
      args:
        - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    container_name: smart-learning-ai
    restart: unless-stopped
    environment:
      # LangChain 설정
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}
      LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-true}
      LANGCHAIN_PROJECT: ${LANGCHAIN_PROJECT:-smart-learning}
      LANGCHAIN_ENDPOINT: https://api.smith.langchain.com
      
      # AI 모델 설정
      MODEL_NAME: ${AI_MODEL_NAME:-gpt-3.5-turbo}
      EMBEDDINGS_MODEL: ${EMBEDDINGS_MODEL:-text-embedding-3-small}
      
      # 서비스 설정
      DATABASE_URL: mysql://${DB_USER:-app_user}:${DB_PASSWORD:-app_password}@database:3306/${DB_NAME:-smart_learning_db}
      REDIS_URL: redis://redis:6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
      LOG_LEVEL: ${AI_LOG_LEVEL:-INFO}
      
      # 성능 설정
      WORKERS: ${AI_WORKERS:-2}
      MAX_CONCURRENT_REQUESTS: ${AI_MAX_CONCURRENT:-10}
    ports:
      - "${AI_PORT:-8000}:8000"
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - ai_data:/app/data
      - ai_uploads:/app/uploaded_documents
      - ai_logs:/app/logs
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      timeout: 30s
      retries: 5
      interval: 30s
      start_period: 120s

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: smart-learning-backend
    restart: unless-stopped
    environment:
      # Spring 설정
      SPRING_PROFILES_ACTIVE: ${SPRING_PROFILES_ACTIVE:-docker}
      
      # 데이터베이스 설정
      DB_HOST: database
      DB_PORT: 3306
      DB_NAME: ${DB_NAME:-smart_learning_db}
      DB_USER: ${DB_USER:-app_user}
      DB_PASSWORD: ${DB_PASSWORD:-app_password}
      
      # Redis 설정
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
      
      # 보안 설정
      JWT_SECRET: ${JWT_SECRET:-mySecretKey123!@#$%^&*()_+}
      
      # AI 서비스 설정
      AI_SERVICE_URL: http://ai-service:8000
      AI_SERVICE_TIMEOUT: 300
      
      # 파일 업로드 설정
      FILE_UPLOAD_MAX_SIZE: 50MB
      FILE_UPLOAD_DIR: /app/uploads
      
      # 모니터링 설정
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,metrics,prometheus
      MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS: always
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
      ai-service:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      timeout: 10s
      retries: 5
      interval: 30s
      start_period: 90s

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - REACT_APP_API_URL=${REACT_APP_API_URL:-http://localhost:8080/api}
        - REACT_APP_AI_URL=${REACT_APP_AI_URL:-http://localhost:8000}
        - NODE_ENV=${NODE_ENV:-production}
    container_name: smart-learning-frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-80}:80"
    depends_on:
      - backend
      - ai-service
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      timeout: 5s
      retries: 3
      interval: 30s
      start_period: 30s

  # 모니터링 - Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: smart-learning-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus_data:/prometheus
    networks:
      - app-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring

  # 모니터링 - Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: smart-learning-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - app-network
    depends_on:
      - prometheus
    profiles:
      - monitoring

  # 로그 수집 - Loki
  loki:
    image: grafana/loki:latest
    container_name: smart-learning-loki
    restart: unless-stopped
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - app-network
    profiles:
      - monitoring

  # 로그 수집 에이전트 - Promtail
  promtail:
    image: grafana/promtail:latest
    container_name: smart-learning-promtail
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yml:ro
      - ai_logs:/var/log/ai:ro
      - backend_logs:/var/log/backend:ro
    networks:
      - app-network
    depends_on:
      - loki
    profiles:
      - monitoring

volumes:
  db_data:
    driver: local
  redis_data:
    driver: local
  backend_uploads:
    driver: local
  backend_logs:
    driver: local
  ai_data:
    driver: local
  ai_uploads:
    driver: local
  ai_logs:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local

networks:
  app-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
    driver_opts:
      com.docker.network.bridge.name: smart-learning-net
```

### docker-compose.dev.yml (개발 환경)
```yaml
version: '3.8'

services:
  ai-service:
    build:
      context: ./ai-module
      dockerfile: Dockerfile
      target: dev
    volumes:
      - ./ai-module/src:/app/src:ro
      - ./ai-module/tests:/app/tests:ro
    environment:
      DEBUG: true
      LOG_LEVEL: DEBUG
      LANGCHAIN_TRACING_V2: true
      RELOAD: true
    command: ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    ports:
      - "8000:8000"
      - "8001:8001"  # 메트릭 포트

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: dev
    volumes:
      - ./backend/src:/app/src:ro
      - ./backend/target:/app/target
    environment:
      SPRING_PROFILES_ACTIVE: dev
      SPRING_DEVTOOLS_RESTART_ENABLED: true
      SPRING_DEVTOOLS_LIVERELOAD_ENABLED: true
    ports:
      - "8080:8080"
      - "35729:35729"  # LiveReload

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    volumes:
      - ./frontend/src:/app/src:ro
      - ./frontend/public:/app/public:ro
    environment:
      NODE_ENV: development
      CHOKIDAR_USEPOLLING: true
      FAST_REFRESH: true
    command: ["npm", "start"]
    ports:
      - "3000:3000"

  # 개발용 도구들
  mailhog:
    image: mailhog/mailhog:latest
    container_name: smart-learning-mailhog
    ports:
      - "1025:1025"  # SMTP
      - "8025:8025"  # Web UI
    networks:
      - app-network

  adminer:
    image: adminer:latest
    container_name: smart-learning-adminer
    ports:
      - "8081:8080"
    networks:
      - app-network
    depends_on:
      - database
```

## 산출물 3: GitHub Actions CI/CD 파이프라인 (최신화)

### .github/workflows/ci-cd.yml
```yaml
name: Smart Learning CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository }}

jobs:
  # 코드 품질 검사 및 테스트
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    strategy:
      matrix:
        component: [backend, frontend, ai-module]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Backend Testing
      if: matrix.component == 'backend'
      uses: actions/setup-java@v4
      with:
        java-version: '17'
        distribution: 'temurin'
        cache: maven

    - name: Setup Frontend Testing
      if: matrix.component == 'frontend'
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Setup AI Module Testing
      if: matrix.component == 'ai-module'
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: ai-module/requirements.txt

    # Backend 테스트
    - name: Run Backend Tests
      if: matrix.component == 'backend'
      run: |
        cd backend
        ./mvnw clean verify -Dspring.profiles.active=test
        ./mvnw jacoco:report

    - name: Upload Backend Coverage
      if: matrix.component == 'backend'
      uses: codecov/codecov-action@v4
      with:
        files: backend/target/site/jacoco/jacoco.xml
        flags: backend
        token: ${{ secrets.CODECOV_TOKEN }}

    # Frontend 테스트
    - name: Install Frontend Dependencies
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm ci --prefer-offline --no-audit

    - name: Run Frontend Lint
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm run lint:check

    - name: Run Frontend Tests
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false --testResultsProcessor=jest-sonar-reporter
      env:
        CI: true

    - name: Upload Frontend Coverage
      if: matrix.component == 'frontend'
      uses: codecov/codecov-action@v4
      with:
        files: frontend/coverage/lcov.info
        flags: frontend
        token: ${{ secrets.CODECOV_TOKEN }}

    # AI Module 테스트
    - name: Install AI Dependencies
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run AI Module Lint
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        black --check src/
        flake8 src/
        mypy src/

    - name: Run AI Module Tests
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        python -m pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Upload AI Coverage
      if: matrix.component == 'ai-module'
      uses: codecov/codecov-action@v4
```      
## 산출물 4: 최신 배포 스크립트

### deploy-k8s.sh (Kubernetes 배포)
```bash
#!/bin/bash
set -euo pipefail

# 색상 정의
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# 로깅 함수
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[STEP]${NC} $1"; }

# 기본 설정
ENVIRONMENT=""
IMAGE_TAG="latest"
DEPLOYMENT_STRATEGY="rolling"
OPENAI_KEY=""
LANGCHAIN_KEY=""
NAMESPACE="smart-learning"
CHART_PATH="./helm/smart-learning"

# 도움말
show_help() {
    cat << EOF
Kubernetes 배포 스크립트

사용법: $0 <환경> [옵션]

환경:
    staging     스테이징 환경
    production  프로덕션 환경

옵션:
    --image-tag TAG         Docker 이미지 태그 (기본값: latest)
    --strategy STRATEGY     배포 전략 (rolling|blue-green|canary)
    --openai-key KEY        OpenAI API 키
    --langchain-key KEY     LangChain API 키
    --namespace NS          Kubernetes 네임스페이스
    --dry-run              실제 배포 없이 시뮬레이션만 실행
    -h, --help             이 도움말 표시

예시:
    $0 staging --image-tag v1.2.3 --strategy rolling
    $0 production --image-tag stable --strategy blue-green

EOF
}

# 파라미터 파싱
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            staging|production)
                ENVIRONMENT=$1
                shift
                ;;
            --image-tag)
                IMAGE_TAG="$2"
                shift 2
                ;;
            --strategy)
                DEPLOYMENT_STRATEGY="$2"
                shift 2
                ;;
            --openai-key)
                OPENAI_KEY="$2"
                shift 2
                ;;
            --langchain-key)
                LANGCHAIN_KEY="$2"
                shift 2
                ;;
            --namespace)
                NAMESPACE="$2"
                shift 2
                ;;
            --dry-run)
                DRY_RUN=true
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option $1"
                show_help
                exit 1
                ;;
        esac
    done

    if [[ -z "$ENVIRONMENT" ]]; then
        log_error "환경을 지정해주세요 (staging 또는 production)"
        show_help
        exit 1
    fi
}

# 사전 조건 확인
check_prerequisites() {
    log_step "사전 조건 확인 중..."

    # 필수 도구 확인
    local tools=("kubectl" "helm")
    for tool in "${tools[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            log_error "$tool이 설치되지 않았습니다."
            exit 1
        fi
    done

    # Kubernetes 연결 확인
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Kubernetes 클러스터에 연결할 수 없습니다."
        exit 1
    fi

    # 네임스페이스 확인/생성
    if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
        log_info "네임스페이스 '$NAMESPACE' 생성 중..."
        kubectl create namespace "$NAMESPACE"
    fi

    # Helm 차트 확인
    if [[ ! -d "$CHART_PATH" ]]; then
        log_error "Helm 차트를 찾을 수 없습니다: $CHART_PATH"
        exit 1
    fi

    log_info "사전 조건 확인 완료"
}

# 시크릿 관리
manage_secrets() {
    log_step "시크릿 관리 중..."

    # AI 서비스 시크릿
    if [[ -n "$OPENAI_KEY" && -n "$LANGCHAIN_KEY" ]]; then
        kubectl create secret generic ai-secrets \
            --namespace="$NAMESPACE" \
            --from-literal=openai-api-key="$OPENAI_KEY" \
            --from-literal=langchain-api-key="$LANGCHAIN_KEY" \
            --dry-run=client -o yaml | kubectl apply -f -
        log_info "AI 서비스 시크릿 업데이트 완료"
    fi

    # 환경별 시크릿 로드
    local secret_file="./k8s/secrets/${ENVIRONMENT}-secrets.yaml"
    if [[ -f "$secret_file" ]]; then
        kubectl apply -f "$secret_file" -n "$NAMESPACE"
        log_info "환경별 시크릿 적용 완료"
    fi
}

# Blue-Green 배포
deploy_blue_green() {
    log_step "Blue-Green 배포 시작..."
    
    local current_version=$(kubectl get deployment backend -n "$NAMESPACE" -o jsonpath='{.metadata.labels.version}' 2>/dev/null || echo "blue")
    local new_version="green"
    
    if [[ "$current_version" == "green" ]]; then
        new_version="blue"
    fi

    log_info "현재 버전: $current_version, 새 버전: $new_version"

    # 새 버전 배포
    helm upgrade --install "smart-learning-$new_version" "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="$new_version" \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=10m

    # 헬스체크
    if ! run_health_checks "$new_version"; then
        log_error "새 버전 헬스체크 실패"
        rollback_deployment "$new_version"
        return 1
    fi

    # 트래픽 전환
    log_info "트래픽을 새 버전으로 전환 중..."
    kubectl patch service frontend -n "$NAMESPACE" \
        -p '{"spec":{"selector":{"version":"'$new_version'"}}}'

    # 이전 버전 정리 (5분 후)
    log_info "5분 후 이전 버전을 정리합니다..."
    sleep 300
    helm uninstall "smart-learning-$current_version" -n "$NAMESPACE" || true

    log_info "Blue-Green 배포 완료"
}

# 카나리 배포
deploy_canary() {
    log_step "카나리 배포 시작..."

    # 현재 버전을 stable로 표시
    kubectl patch deployment backend -n "$NAMESPACE" \
        -p '{"metadata":{"labels":{"version":"stable"}}}'

    # 카나리 버전 배포 (10% 트래픽)
    helm upgrade --install smart-learning-canary "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="canary" \
        --set global.replica.count=1 \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=10m

    # 트래픽 분할 설정 (Istio 사용 가정)
    kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: smart-learning-vs
  namespace: $NAMESPACE
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: frontend
        subset: canary
  - route:
    - destination:
        host: frontend
        subset: stable
      weight: 90
    - destination:
        host: frontend
        subset: canary
      weight: 10
EOF

    log_info "카나리 배포 완료 (10% 트래픽)"

    # 30분 대기 후 전체 트래픽 전환
    log_info "30분 후 전체 트래픽을 새 버전으로 전환합니다..."
    sleep 1800

    if run_health_checks "canary"; then
        # 전체 트래픽을 카나리로 전환
        kubectl patch virtualservice smart-learning-vs -n "$NAMESPACE" \
            --type='json' \
            -p='[{"op": "replace", "path": "/spec/http/1/route/0/weight", "value": 0}]'
        kubectl patch virtualservice smart-learning-vs -n "$NAMESPACE" \
            --type='json' \
            -p='[{"op": "replace", "path": "/spec/http/1/route/1/weight", "value": 100}]'
        
        log_info "카나리 배포 성공"
    else
        log_error "카나리 버전 헬스체크 실패, 롤백 중..."
        rollback_deployment "canary"
    fi
}

# 롤링 배포
deploy_rolling() {
    log_step "롤링 배포 시작..."

    helm upgrade --install smart-learning "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="stable" \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=15m

    log_info "롤링 배포 완료"
}

# 헬스체크
run_health_checks() {
    local version=${1:-"stable"}
    log_step "헬스체크 실행 중... (버전: $version)"

    local max_attempts=30
    local attempt=0

    while [[ $attempt -lt $max_attempts ]]; do
        attempt=$((attempt + 1))

        # 모든 포드가 Ready 상태인지 확인
        local ready_pods=$(kubectl get pods -n "$NAMESPACE" \
            -l version="$version" \
            -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
            grep -c "True" || echo "0")
        
        local total_pods=$(kubectl get pods -n "$NAMESPACE" \
            -l version="$version" \
            --no-headers | wc -l)

        if [[ "$ready_pods" -eq "$total_pods" && "$total_pods" -gt 0 ]]; then
            log_info "모든 포드가 Ready 상태입니다."
            
            # 애플리케이션 헬스체크
            local service_url=$(kubectl get service frontend -n "$NAMESPACE" \
                -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "localhost")
            
            if curl -f -s "http://$service_url/health" > /dev/null; then
                log_info "애플리케이션 헬스체크 성공"
                return 0
            fi
        fi

        log_warn "헬스체크 재시도 ($attempt/$max_attempts)..."
        sleep 30
    done

    log_error "헬스체크 실패"
    return 1
}

# 롤백
rollback_deployment() {
    local version=${1:-""}
    log_step "배포 롤백 중..."

    if [[ -n "$version" ]]; then
        helm rollback "smart-learning-$version" -n "$NAMESPACE"
    else
        helm rollback smart-learning -n "$NAMESPACE"
    fi

    log_info "롤백 완료"
}

# 배포 후 검증
post_deployment_verification() {
    log_step "배포 후 검증 중..."

    # 기본 API 엔드포인트 테스트
    local api_tests=(
        "/health"
        "/api/actuator/health"
        "/ai/health"
    )

    local service_url=$(kubectl get ingress smart-learning-ingress -n "$NAMESPACE" \
        -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "localhost")

    for endpoint in "${api_tests[@]}"; do
        if curl -f -s "https://$service_url$endpoint" > /dev/null; then
            log_info "✓ $endpoint 엔드포인트 정상"
        else
            log_warn "✗ $endpoint 엔드포인트 실패"
        fi
    done

    # 메트릭 확인
    if kubectl get pods -n "$NAMESPACE" -l app=prometheus &> /dev/null; then
        log_info "모니터링 스택 확인 중..."
        # Prometheus 메트릭 확인 로직
    fi

    log_info "배포 후 검증 완료"
}

# 메인 함수
main() {
    log_info "Smart Learning Kubernetes 배포 시작"
    
    parse_args "$@"
    
    log_info "환경: $ENVIRONMENT"
    log_info "이미지 태그: $IMAGE_TAG" 
    log_info "배포 전략: $DEPLOYMENT_STRATEGY"
    log_info "네임스페이스: $NAMESPACE"

    if [[ "${DRY_RUN:-false}" == "true" ]]; then
        log_warn "DRY RUN 모드 - 실제 배포는 수행되지 않습니다."
        exit 0
    fi

    check_prerequisites
    manage_secrets

    # 배포 전략에 따른 실행
    case "$DEPLOYMENT_STRATEGY" in
        "blue-green")
            deploy_blue_green
            ;;
        "canary")
            deploy_canary
            ;;
        "rolling"|*)
            deploy_rolling
            ;;
    esac

    run_health_checks
    post_deployment_verification

    log_info "배포 완료! 🎉"
    kubectl get pods -n "$NAMESPACE"
}

# 스크립트 실행
main "$@"
```

### docker-deploy.sh (Docker Compose 배포)
```bash
#!/bin/bash
set -euo pipefail

# 색상 정의
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# 로깅 함수
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[STEP]${NC} $1"; }

# 기본 설정
ENVIRONMENT="production"
COMPOSE_PROJECT_NAME="smart-learning"
BACKUP_ENABLED=true
MONITORING_ENABLED=false
BUILD_IMAGES=false
PULL_IMAGES=true

# 도움말
show_help() {
    cat << EOF
Docker Compose 배포 스크립트

사용법: $0 [옵션]

옵션:
    -e, --environment ENV   배포 환경 (dev|staging|production)
    -b, --build            로컬에서 이미지 빌드
    -p, --pull             레지스트리에서 이미지 풀 (기본값)
    -m, --monitoring       모니터링 스택 포함
    --no-backup           백업 건너뛰기
    --cleanup             기존 컨테이너 정리 후 배포
    -h, --help            이 도움말 표시

예시:
    $0 -e staging -m
    $0 --build --monitoring
    $0 --cleanup -e production

EOF
}

# 파라미터 파싱
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -e|--environment)
                ENVIRONMENT="$2"
                shift 2
                ;;
            -b|--build)
                BUILD_IMAGES=true
                PULL_IMAGES=false
                shift
                ;;
            -p|--pull)
                PULL_IMAGES=true
                BUILD_IMAGES=false
                shift
                ;;
            -m|--monitoring)
                MONITORING_ENABLED=true
                shift
                ;;
            --no-backup)
                BACKUP_ENABLED=false
                shift
                ;;
            --cleanup)
                CLEANUP_ENABLED=true
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# 환경 설정
setup_environment() {
    log_step "환경 설정 중..."

    # 환경별 설정 파일 선택
    case "$ENVIRONMENT" in
        "dev"|"development")
            COMPOSE_FILE="docker-compose.dev.yml"
            ENV_FILE=".env.development"
            ;;
        "staging")
            COMPOSE_FILE="docker-compose.staging.yml"
            ENV_FILE=".env.staging"
            ;;
        "production")
            COMPOSE_FILE="docker-compose.yml"
            ENV_FILE=".env.production"
            ;;
        *)
            log_error "지원하지 않는 환경: $ENVIRONMENT"
            exit 1
            ;;
    esac

    # 환경 파일 확인
    if [[ ! -f "$ENV_FILE" ]]; then
        log_error "환경 파일을 찾을 수 없습니다: $ENV_FILE"
        exit 1
    fi

    # Compose 파일 확인
    if [[ ! -f "$COMPOSE_FILE" ]]; then
        log_error "Compose 파일을 찾을 수 없습니다: $COMPOSE_FILE"
        exit 1
    fi

    # 모니터링 설정
    if [[ "$MONITORING_ENABLED" == "true" ]]; then
        COMPOSE_PROFILES="--profile monitoring"
    else
        COMPOSE_PROFILES=""
    fi

    log_info "환경: $ENVIRONMENT"
    log_info "Compose 파일: $COMPOSE_FILE"
    log_info "환경 파일: $ENV_FILE"
    log_info "모니터링: $MONITORING_ENABLED"
}

# 사전 조건 확인
check_prerequisites() {
    log_step "사전 조건 확인 중..."

    # Docker 확인
    if !# Day 5: 컨테이너화 및 CI/CD (최신 LangServe 통합)     
```

## 산출물 5: 최신 모니터링 설정

### monitoring/prometheus.yml (LangServe 메트릭 포함)
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'smart-learning'
    environment: '${ENVIRONMENT:-production}'

rule_files:
  - "alert_rules.yml"
  - "langchain_rules.yml"

scrape_configs:
  # Backend 서비스 (Spring Boot Actuator)
  - job_name: 'smart-learning-backend'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['backend:8080']
    scrape_interval: 30s
    scrape_timeout: 10s

  # AI 서비스 (LangServe + Custom Metrics)
  - job_name: 'smart-learning-ai'
    static_configs:
      - targets: ['ai-service:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 15s

  # LangChain 추적 메트릭
  - job_name: 'langchain-metrics'
    static_configs:
      - targets: ['ai-service:8001']  # 별도 메트릭 포트
    metrics_path: '/langchain/metrics'
    scrape_interval: 60s

  # Frontend 서비스
  - job_name: 'smart-learning-frontend'
    static_configs:
      - targets: ['frontend:80']
    metrics_path: '/metrics'
    scrape_interval: 60s

  # 데이터베이스 메트릭
  - job_name: 'mysql-exporter'
    static_configs:
      - targets: ['mysql-exporter:9104']
    scrape_interval: 30s

  # Redis 메트릭
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 30s

  # Node 메트릭
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 30s

  # Docker 메트릭
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 30s

  # Nginx 메트릭
  - job_name: 'nginx-exporter'
    static_configs:
      - targets: ['nginx-exporter:9113']
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - 'alertmanager:9093'

# 스토리지 설정
storage:
  tsdb:
    retention.time: 30d
    retention.size: 10GB
```

### monitoring/langchain_rules.yml (LangChain 전용 알림)
```yaml
groups:
  - name: langchain-alerts
    rules:
      # AI 서비스 응답 시간
      - alert: LangChainHighLatency
        expr: histogram_quantile(0.95, rate(langchain_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "LangChain 응답 시간이 높습니다"
          description: "95th percentile 응답 시간이 {{ $value }}초입니다"

      # AI 모델 신뢰도
      - alert: LowAIConfidence
        expr: avg_over_time(langchain_response_confidence[15m]) < 0.5
        for: 10m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "AI 응답 신뢰도가 낮습니다"
          description: "평균 신뢰도가 {{ $value | humanizePercentage }}입니다"

      # 벡터 스토어 크기
      - alert: VectorStoreGrowth
        expr: increase(langchain_vector_store_documents_total[1h]) > 1000
        for: 0s
        labels:
          severity: info
          service: ai-service
        annotations:
          summary: "벡터 스토어 빠른 증가"
          description: "지난 1시간 동안 {{ $value }}개 문서 추가됨"

      # OpenAI API 오류율
      - alert: OpenAIAPIErrors
        expr: rate(langchain_openai_api_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          service: ai-service
        annotations:
          summary: "OpenAI API 오류율 높음"
          description: "OpenAI API 오류율이 {{ $value | humanizePercentage }}입니다"

      # 토큰 사용량 급증
      - alert: HighTokenUsage
        expr: rate(langchain_token_usage_total[5m]) > 10000
        for: 5m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "토큰 사용량 급증"
          description: "분당 {{ $value }}개 토큰 사용 중"

  - name: smart-learning-business
    rules:
      # 사용자 활동
      - alert: LowUserActivity
        expr: rate(smart_learning_user_sessions_total[1h]) < 5
        for: 30m
        labels:
          severity: warning
          service: backend
        annotations:
          summary: "사용자 활동 감소"
          description: "시간당 {{ $value }}개 세션만 활성화됨"

      # 학습 완료율
      - alert: LowCompletionRate
        expr: (rate(smart_learning_course_completions_total[24h]) / rate(smart_learning_course_starts_total[24h])) < 0.3
        for: 2h
        labels:
          severity: warning
          service: backend
        annotations:
          summary: "학습 완료율 저하"
          description: "24시간 완료율이 {{ $value | humanizePercentage }}입니다"
```

### monitoring/grafana/dashboards/smart-learning-overview.json
```json
{
  "dashboard": {
    "id": null,
    "title": "Smart Learning Overview",
    "tags": ["smart-learning", "overview"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Service Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=~\"smart-learning-.*\"}",
            "legendFormat": "{{job}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "AI Response Time (95th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(langchain_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "User Activity",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(smart_learning_user_sessions_total[5m]) * 60",
            "legendFormat": "Sessions per minute"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "AI Confidence Score",
        "type": "stat",
        "targets": [
          {
            "expr": "avg_over_time(langchain_response_confidence[5m])",
            "legendFormat": "Average Confidence"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.5},
                {"color": "green", "value": 0.8}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```
    
## 산출물 6: 최신 환경 설정

### .env.production (LangServe 통합)
```env
# 애플리케이션 환경
ENVIRONMENT=production
COMPOSE_PROJECT_NAME=smart-learning
NODE_ENV=production

# 데이터베이스 설정
DB_NAME=smart_learning_prod
DB_USER=prod_user
DB_PASSWORD=SuperSecurePassword123!@#
DB_ROOT_PASSWORD=RootSecurePassword456!@#
DB_PORT=3306

# Redis 설정
REDIS_PASSWORD=RedisSecurePassword789!@#
REDIS_PORT=6379

# 보안 설정
JWT_SECRET=ProductionJWTSecretKey_MustBe256BitsLong_123456789012345678901234567890

# AI 서비스 설정 (LangServe)
OPENAI_API_KEY=sk-your-openai-api-key-here
LANGCHAIN_API_KEY=your-langchain-api-key-here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=smart-learning-production
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# AI 모델 설정
AI_MODEL_NAME=gpt-3.5-turbo
EMBEDDINGS_MODEL=text-embedding-3-small
AI_WORKERS=2
AI_MAX_CONCURRENT=10
AI_LOG_LEVEL=INFO

# 서비스 포트
BACKEND_PORT=8080
FRONTEND_PORT=80
AI_PORT=8000
REDIS_PORT=6379
DB_PORT=3306

# React 환경변수
REACT_APP_API_URL=https://api.smartlearning.com/api
REACT_APP_AI_URL=https://ai.smartlearning.com

# Spring Boot 설정
SPRING_PROFILES_ACTIVE=production
MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE=health,info,metrics,prometheus
MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS=when_authorized

# 모니터링
PROMETHEUS_PORT=9090
GRAFANA_PORT=3001
GRAFANA_PASSWORD=SecureGrafanaPassword!@#

# 알림 설정
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
NOTIFICATION_EMAIL=admin@smartlearning.com

# 백업 설정
BACKUP_SCHEDULE=0 2 * * *
BACKUP_RETENTION_DAYS=30
BACKUP_S3_BUCKET=smart-learning-backups
AWS_REGION=ap-northeast-2

# 성능 튜닝
JAVA_OPTS=-Xmx2g -Xms1g -XX:+UseG1GC -XX:+UseContainerSupport
PYTHON_WORKERS=2
NGINX_WORKER_PROCESSES=auto
```

### .env.development (개발환경)
```env
# 개발 환경 설정
ENVIRONMENT=development
COMPOSE_PROJECT_NAME=smart-learning-dev
NODE_ENV=development

# 데이터베이스 설정
DB_NAME=smart_learning_dev
DB_USER=dev_user
DB_PASSWORD=dev_password
DB_ROOT_PASSWORD=dev_root_password
DB_PORT=3307

# Redis 설정
REDIS_PASSWORD=dev_redis_password
REDIS_PORT=6380

# 개발용 보안 설정
JWT_SECRET=dev_jwt_secret_key_for_development_only

# AI 서비스 설정
OPENAI_API_KEY=sk-your-dev-openai-api-key
LANGCHAIN_API_KEY=your-dev-langchain-api-key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=smart-learning-development

# AI 모델 설정 (개발용)
AI_MODEL_NAME=gpt-3.5-turbo
EMBEDDINGS_MODEL=text-embedding-3-small
AI_WORKERS=1
AI_MAX_CONCURRENT=5
AI_LOG_LEVEL=DEBUG

# 개발 서버 포트
BACKEND_PORT=8081
FRONTEND_PORT=3000
AI_PORT=8001

# React 개발 설정
REACT_APP_API_URL=http://localhost:8081/api
REACT_APP_AI_URL=http://localhost:8001
CHOKIDAR_USEPOLLING=true
FAST_REFRESH=true

# Spring Boot 개발 설정
SPRING_PROFILES_ACTIVE=dev
SPRING_DEVTOOLS_RESTART_ENABLED=true
SPRING_DEVTOOLS_LIVERELOAD_ENABLED=true

# 개발 도구
PROMETHEUS_PORT=9091
GRAFANA_PORT=3002
GRAFANA_PASSWORD=admin

# 디버깅
DEBUG=true
LOG_LEVEL=DEBUG
```