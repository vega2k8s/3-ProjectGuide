# Day 4: AI ëª¨ë“ˆ í†µí•© (ìµœì‹  LangChain/LangGraph ë²„ì „)

## ì‚°ì¶œë¬¼ 1: ìµœì‹  AI ëª¨ë“ˆ êµ¬ì¡°

```
ai-module/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â””â”€â”€ embeddings_manager.py
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ qa_chain.py
â”‚   â”‚   â”œâ”€â”€ quiz_generator.py
â”‚   â”‚   â””â”€â”€ summarizer.py
â”‚   â”œâ”€â”€ graphs/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag_graph.py
â”‚   â”‚   â””â”€â”€ corrective_rag.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ logger.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ tests/
    â”œâ”€â”€ test_chains.py
    â””â”€â”€ test_api.py
```

## ì‚°ì¶œë¬¼ 2: ìµœì‹  RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### 1. ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© (core/document_processor.py)
```python
import os
import hashlib
from typing import List, Dict, Any, Optional
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import logging

logger = logging.getLogger(__name__)

class DocumentProcessor:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
    def load_document(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> List[Document]:
        """íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        file_extension = os.path.splitext(file_path)[1].lower()
        
        try:
            if file_extension == '.pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension in ['.txt', '.md']:
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_extension == '.docx':
                loader = Docx2txtLoader(file_path)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
            
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in documents:
                if metadata:
                    doc.metadata.update(metadata)
                doc.metadata['file_path'] = file_path
                doc.metadata['file_type'] = file_extension
                doc.metadata['content_hash'] = self._generate_content_hash(doc.page_content)
            
            return documents
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {str(e)}")
            raise
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        try:
            chunks = self.text_splitter.split_documents(documents)
            
            # ì²­í¬ì— ì¸ë±ìŠ¤ ì¶”ê°€
            for i, chunk in enumerate(chunks):
                chunk.metadata['chunk_index'] = i
                chunk.metadata['chunk_size'] = len(chunk.page_content)
            
            logger.info(f"ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¶„í•  ì‹¤íŒ¨: {str(e)}")
            raise
    
    def process_file(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> List[Document]:
        """íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë¶„í• í•©ë‹ˆë‹¤."""
        documents = self.load_document(file_path, metadata)
        chunks = self.split_documents(documents)
        return chunks
    
    def _generate_content_hash(self, content: str) -> str:
        """ì½˜í…ì¸ ì˜ í•´ì‹œê°’ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        return hashlib.md5(content.encode()).hexdigest()
```

### 2. ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ì (core/vector_store.py)
```python
import os
from typing import List, Optional, Dict, Any
from langchain_openai import OpenAIEmbeddings
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import logging

logger = logging.getLogger(__name__)

class VectorStoreManager:
    def __init__(self, 
                 embedding_provider: str = "openai",
                 model_name: str = "text-embedding-3-small"):
        self.embedding_provider = embedding_provider
        self.model_name = model_name
        
        if embedding_provider == "openai":
            self.embeddings = OpenAIEmbeddings(model=model_name)
        elif embedding_provider == "ollama":
            self.embeddings = OllamaEmbeddings(model=model_name)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ì œê³µì: {embedding_provider}")
            
        self.vector_store: Optional[FAISS] = None
        self.store_path = "data/vector_store"
        
    def load_or_create_store(self) -> Optional[FAISS]:
        """ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤."""
        if os.path.exists(f"{self.store_path}/index.faiss"):
            try:
                self.vector_store = FAISS.load_local(
                    self.store_path, 
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.info("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                self.vector_store = None
        
        return self.vector_store
    
    def add_documents(self, documents: List[Document], content_id: Optional[int] = None):
        """ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # ë¬¸ì„œì— content_id ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if content_id:
                for doc in documents:
                    doc.metadata['content_id'] = content_id
            
            if self.vector_store is None:
                self.vector_store = FAISS.from_documents(documents, self.embeddings)
                logger.info(f"ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±: {len(documents)}ê°œ ë¬¸ì„œ")
            else:
                self.vector_store.add_documents(documents)
                logger.info(f"ë²¡í„° ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€: {len(documents)}ê°œ")
            
            self.save_store()
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def similarity_search(self, query: str, k: int = 5, 
                         content_id: Optional[int] = None) -> List[Document]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if self.vector_store is None:
            return []
        
        try:
            if content_id:
                # íŠ¹ì • ì½˜í…ì¸ ì—ì„œë§Œ ê²€ìƒ‰
                docs = self.vector_store.similarity_search(query, k=k*2)  # ë” ë§ì´ ê²€ìƒ‰ í›„ í•„í„°ë§
                filtered_docs = [doc for doc in docs if doc.metadata.get('content_id') == content_id]
                results = filtered_docs[:k]
            else:
                results = self.vector_store.similarity_search(query, k=k)
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def save_store(self):
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        if self.vector_store:
            os.makedirs(self.store_path, exist_ok=True)
            self.vector_store.save_local(self.store_path)
            logger.info("ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ")
```

### 3. ìµœì‹  QA ì²´ì¸ (chains/qa_chain.py)
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.output_parsers import StrOutputParser
from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger(__name__)

class SmartQAChain:
    def __init__(self, 
                 vector_store_manager,
                 llm_provider: str = "openai",
                 model_name: str = "gpt-3.5-turbo",
                 temperature: float = 0.3):
        
        self.vector_store_manager = vector_store_manager
        
        # LLM ì´ˆê¸°í™”
        if llm_provider == "openai":
            self.llm = ChatOpenAI(
                model=model_name,
                temperature=temperature
            )
        elif llm_provider == "ollama":
            self.llm = ChatOllama(
                model=model_name,
                temperature=temperature
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {llm_provider}")
        
        # í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì •ë³´ë‚˜ ì˜ˆì‹œê°€ ìˆë‹¤ë©´ í¬í•¨í•˜ì„¸ìš”

ë¬¸ì„œ ë‚´ìš©:
{context}"""),
            ("human", "{input}")
        ])
        
        # ë¬¸ì„œ ì²´ì¸ ìƒì„±
        self.document_chain = create_stuff_documents_chain(self.llm, self.prompt)
    
    def ask_question(self, question: str, content_id: Optional[int] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self.vector_store_manager.similarity_search(
                question, k=5, content_id=content_id
            )
            
            if not relevant_docs:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    "sources": [],
                    "confidence": 0.0
                }
            
            # Retriever ìƒì„±
            retriever = self.vector_store_manager.vector_store.as_retriever(
                search_kwargs={"k": 5}
            )
            
            # Retrieval ì²´ì¸ ìƒì„±
            retrieval_chain = create_retrieval_chain(retriever, self.document_chain)
            
            # ë‹µë³€ ìƒì„±
            response = retrieval_chain.invoke({"input": question})
            
            # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            sources = []
            for doc in response.get("context", []):
                source_info = {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata
                }
                sources.append(source_info)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(question, relevant_docs, response["answer"])
            
            return {
                "answer": response["answer"],
                "sources": sources,
                "confidence": confidence
            }
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "confidence": 0.0
            }
    
    def _calculate_confidence(self, question: str, docs: List, answer: str) -> float:
        """ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not docs or not answer:
            return 0.0
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ì‹ ë¢°ë„
        question_words = set(question.lower().split())
        doc_words = set()
        for doc in docs:
            doc_words.update(doc.page_content.lower().split())
        
        overlap = len(question_words.intersection(doc_words))
        confidence = min(overlap / len(question_words) if question_words else 0, 1.0)
        
        # ë‹µë³€ ê¸¸ì´ ê³ ë ¤ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‹µë³€ì€ ì‹ ë¢°ë„ í•˜ë½)
        answer_length_factor = min(len(answer) / 100, 1.0)
        confidence *= answer_length_factor
        
        return round(min(confidence, 1.0), 2)
```

### 4. LangGraph RAG êµ¬í˜„ (graphs/rag_graph.py)
```python
from langgraph import StateGraph, START, END
from typing import TypedDict, List, Annotated, Optional
from operator import add
from langchain_core.documents import Document
import logging

logger = logging.getLogger(__name__)

class RAGState(TypedDict):
    question: str
    documents: Annotated[List[Document], add]
    answer: str
    confidence: float
    content_id: Optional[int]
    retrieval_count: int

class SmartRAGGraph:
    def __init__(self, qa_chain, vector_store_manager, max_retrievals: int = 2):
        self.qa_chain = qa_chain
        self.vector_store_manager = vector_store_manager
        self.max_retrievals = max_retrievals
        self.graph = self._build_graph()
    
    def _build_graph(self):
        """RAG ì²˜ë¦¬ë¥¼ ìœ„í•œ ìƒíƒœ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        workflow = StateGraph(RAGState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("retrieve", self._retrieve_documents)
        workflow.add_node("grade", self._grade_documents)
        workflow.add_node("generate", self._generate_answer)
        workflow.add_node("rewrite", self._rewrite_question)
        
        # ì—£ì§€ ì„¤ì •
        workflow.add_edge(START, "retrieve")
        workflow.add_edge("retrieve", "grade")
        
        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "grade",
            self._decide_next_step,
            {
                "rewrite": "rewrite",
                "generate": "generate",
            }
        )
        workflow.add_edge("rewrite", "retrieve")
        workflow.add_edge("generate", END)
        
        return workflow.compile()
    
    def _retrieve_documents(self, state: RAGState) -> RAGState:
        """ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        logger.info("--- ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ ---")
        
        question = state["question"]
        content_id = state.get("content_id")
        
        documents = self.vector_store_manager.similarity_search(
            question, k=5, content_id=content_id
        )
        
        return {
            **state,
            "documents": documents,
            "retrieval_count": state.get("retrieval_count", 0) + 1
        }
    
    def _grade_documents(self, state: RAGState) -> RAGState:
        """ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        logger.info("--- ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ---")
        
        question = state["question"]
        documents = state["documents"]
        
        if not documents:
            return {**state, "confidence": 0.0}
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        question_words = set(question.lower().split())
        relevant_docs = []
        
        for doc in documents:
            doc_words = set(doc.page_content.lower().split())
            overlap = len(question_words.intersection(doc_words))
            
            # ê´€ë ¨ì„±ì´ ìˆëŠ” ë¬¸ì„œë§Œ ì„ íƒ
            if overlap > 0:
                relevant_docs.append(doc)
        
        confidence = len(relevant_docs) / len(documents) if documents else 0
        
        return {
            **state,
            "documents": relevant_docs,
            "confidence": confidence
        }
    
    def _decide_next_step(self, state: RAGState) -> str:
        """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        confidence = state.get("confidence", 0)
        retrieval_count = state.get("retrieval_count", 0)
        
        # ì‹ ë¢°ë„ê°€ ë‚®ê³  ì¬ê²€ìƒ‰ íšŸìˆ˜ê°€ í•œê³„ ë¯¸ë§Œì´ë©´ ì§ˆë¬¸ ì¬ì‘ì„±
        if confidence < 0.3 and retrieval_count < self.max_retrievals:
            return "rewrite"
        else:
            return "generate"
    
    def _rewrite_question(self, state: RAGState) -> RAGState:
        """ì§ˆë¬¸ì„ ì¬ì‘ì„±í•©ë‹ˆë‹¤."""
        logger.info("--- ì§ˆë¬¸ ì¬ì‘ì„± ---")
        
        question = state["question"]
        
        # ê°„ë‹¨í•œ ì§ˆë¬¸ ì¬ì‘ì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ LLM ê¸°ë°˜ ì¬ì‘ì„± ê°€ëŠ¥)
        rewritten_question = f"ë‹¤ìŒ ë‚´ìš©ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”: {question}"
        
        return {
            **state,
            "question": rewritten_question
        }
    
    def _generate_answer(self, state: RAGState) -> RAGState:
        """ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.info("--- ë‹µë³€ ìƒì„± ---")
        
        question = state["question"]
        content_id = state.get("content_id")
        
        result = self.qa_chain.ask_question(question, content_id)
        
        return {
            **state,
            "answer": result["answer"],
            "confidence": max(state.get("confidence", 0), result["confidence"])
        }
    
    def process_question(self, question: str, content_id: Optional[int] = None) -> Dict[str, Any]:
        """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        initial_state = {
            "question": question,
            "documents": [],
            "answer": "",
            "confidence": 0.0,
            "content_id": content_id,
            "retrieval_count": 0
        }
        
        try:
            final_state = self.graph.invoke(initial_state)
            
            return {
                "question": question,
                "answer": final_state["answer"],
                "confidence": final_state["confidence"],
                "sources": [doc.metadata for doc in final_state["documents"][:3]]
            }
        except Exception as e:
            logger.error(f"RAG ê·¸ë˜í”„ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "question": question,
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "confidence": 0.0,
                "sources": []
            }
```

## ì‚°ì¶œë¬¼ 3: LangServe API ì„œë²„ êµ¬í˜„

### 1. ë©”ì¸ API ì„œë²„ (api/main.py)
```python
from fastapi import FastAPI
from langserve import add_routes
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
app = FastAPI(
    title="Smart Learning AI API",
    description="AI ê¸°ë°˜ í•™ìŠµ ì§€ì› ì‹œìŠ¤í…œ API",
    version="1.0.0"
)

# ì „ì—­ ë³€ìˆ˜ë¡œ AI ì»´í¬ë„ŒíŠ¸ ê´€ë¦¬
from src.core.document_processor import DocumentProcessor
from src.core.vector_store import VectorStoreManager
from src.chains.qa_chain import SmartQAChain
from src.graphs.rag_graph import SmartRAGGraph

# AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
document_processor = DocumentProcessor()
vector_store_manager = VectorStoreManager(embedding_provider="openai")
qa_chain = None
rag_graph = None

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global qa_chain, rag_graph
    
    # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    vector_store_manager.load_or_create_store()
    
    # ì²´ì¸ ì´ˆê¸°í™”
    qa_chain = SmartQAChain(vector_store_manager)
    rag_graph = SmartRAGGraph(qa_chain, vector_store_manager)
    
    print("AI ëª¨ë“ˆ ì´ˆê¸°í™” ì™„ë£Œ")

# ê¸°ë³¸ QA ì²´ì¸ì„ ìœ„í•œ Runnable ìƒì„±
def create_qa_runnable():
    """QAë¥¼ ìœ„í•œ Runnable ì²´ì¸ ìƒì„±"""
    def qa_function(inputs):
        question = inputs.get("question", inputs.get("input", ""))
        content_id = inputs.get("content_id")
        
        if qa_chain:
            result = qa_chain.ask_question(question, content_id)
            return {
                "answer": result["answer"],
                "confidence": result["confidence"],
                "sources": result["sources"]
            }
        else:
            return {"answer": "AI ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "confidence": 0.0, "sources": []}
    
    return RunnableLambda(qa_function)

# RAG ê·¸ë˜í”„ë¥¼ ìœ„í•œ Runnable ìƒì„±
def create_rag_runnable():
    """RAG ê·¸ë˜í”„ë¥¼ ìœ„í•œ Runnable ì²´ì¸ ìƒì„±"""
    def rag_function(inputs):
        question = inputs.get("question", inputs.get("input", ""))
        content_id = inputs.get("content_id")
        
        if rag_graph:
            result = rag_graph.process_question(question, content_id)
            return result
        else:
            return {"answer": "AI ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "confidence": 0.0, "sources": []}
    
    return RunnableLambda(rag_function)

# ê°„ë‹¨í•œ ì±„íŒ… ì²´ì¸
chat_prompt = ChatPromptTemplate.from_template("ì§ˆë¬¸: {question}\në‹µë³€:")
chat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
chat_chain = chat_prompt | chat_model | StrOutputParser()

# LangServe ë¼ìš°íŠ¸ ì¶”ê°€
add_routes(app, chat_chain, path="/chat")
add_routes(app, create_qa_runnable(), path="/qa")
add_routes(app, create_rag_runnable(), path="/rag")

# ì¶”ê°€ API ì—”ë“œí¬ì¸íŠ¸ë“¤
from fastapi import UploadFile, File, HTTPException
from pydantic import BaseModel
import shutil
from typing import Optional, List

class QueryRequest(BaseModel):
    question: str
    content_id: Optional[int] = None
    use_rag_graph: bool = True

class UploadResponse(BaseModel):
    message: str
    content_id: int
    chunks_processed: int

@app.post("/upload-document", response_model=UploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    title: str = "",
    category: str = "",
    content_id: int = 0
):
    """ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ ì €ì¥
        upload_dir = "uploaded_documents"
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, f"{content_id}_{file.filename}")
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "title": title or file.filename,
            "category": category,
            "content_id": content_id,
            "filename": file.filename
        }
        
        # ë¬¸ì„œ ì²˜ë¦¬
        chunks = document_processor.process_file(file_path, metadata)
        
        # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
        vector_store_manager.add_documents(chunks, content_id)
        
        # QA ì²´ì¸ ì¬ì´ˆê¸°í™”
        global qa_chain, rag_graph
        qa_chain = SmartQAChain(vector_store_manager)
        rag_graph = SmartRAGGraph(qa_chain, vector_store_manager)
        
        return UploadResponse(
            message="ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.",
            content_id=content_id,
            chunks_processed=len(chunks)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    return {
        "status": "healthy",
        "vector_store_ready": vector_store_manager.vector_store is not None,
        "qa_chain_ready": qa_chain is not None
    }

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. í€´ì¦ˆ ìƒì„±ê¸° (chains/quiz_generator.py)
```python
import json
from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
import logging

logger = logging.getLogger(__name__)

class QuizQuestion(BaseModel):
    question: str = Field(description="í€´ì¦ˆ ë¬¸ì œ")
    options: List[str] = Field(description="4ê°œì˜ ì„ íƒì§€")
    correct_answer: str = Field(description="ì •ë‹µ")
    explanation: str = Field(description="í•´ì„¤")
    difficulty: str = Field(description="ë‚œì´ë„ (ì‰¬ì›€/ë³´í†µ/ì–´ë ¤ì›€)")

class QuizResponse(BaseModel):
    questions: List[QuizQuestion] = Field(description="ìƒì„±ëœ í€´ì¦ˆ ë¬¸ì œë“¤")

class QuizGenerator:
    def __init__(self, llm, vector_store_manager):
        self.llm = llm
        self.vector_store_manager = vector_store_manager
        
        # JSON íŒŒì„œ ì„¤ì •
        self.parser = JsonOutputParser(pydantic_object=QuizResponse)
        
        self.quiz_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í•™ìŠµ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ê° ë¬¸ì œëŠ” 4ê°œì˜ ì„ íƒì§€ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
2. ì •ë‹µì€ í•˜ë‚˜ë§Œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤  
3. ë¬¸ì œëŠ” í•™ìŠµ ë‚´ìš©ì˜ í•µì‹¬ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
4. ì„ íƒì§€ëŠ” ëª…í™•í•˜ê³  êµ¬ë³„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
5. í•´ì„¤ì€ ì •ë‹µì˜ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤

{format_instructions}

í•™ìŠµ ë‚´ìš©:
{content}

ë‚œì´ë„: {difficulty}
ë¬¸ì œ ìˆ˜: {num_questions}ê°œ"""),
            ("human", "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í€´ì¦ˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        ])
    
    def generate_quiz(self, content_id: int, difficulty: str = "ë³´í†µ", 
                     num_questions: int = 5) -> List[Dict[str, Any]]:
        """ì½˜í…ì¸  ê¸°ë°˜ í€´ì¦ˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì½˜í…ì¸ ì—ì„œ ëŒ€í‘œì ì¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            docs = self.vector_store_manager.similarity_search(
                "ì£¼ìš” ë‚´ìš© í•µì‹¬ ê°œë…", k=3, content_id=content_id
            )
            
            if not docs:
                return []
            
            content = "\n\n".join([doc.page_content for doc in docs])
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‹¤í–‰
            chain = self.quiz_prompt | self.llm | self.parser
            
            result = chain.invoke({
                "content": content[:2000],  # í† í° ìˆ˜ ì œí•œ
                "difficulty": difficulty,
                "num_questions": num_questions,
                "format_instructions": self.parser.get_format_instructions()
            })
            
            # ê²°ê³¼ ê²€ì¦ ë° ë°˜í™˜
            questions = result.get("questions", [])
            validated_questions = []
            
            for q in questions:
                if self._validate_question(q):
                    validated_questions.append(q.dict() if hasattr(q, 'dict') else q)
            
            logger.info(f"í€´ì¦ˆ ìƒì„± ì™„ë£Œ: {len(validated_questions)}ê°œ ë¬¸ì œ")
            return validated_questions
            
        except Exception as e:
            logger.error(f"í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _validate_question(self, question) -> bool:
        """í€´ì¦ˆ ë¬¸ì œì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        if hasattr(question, 'dict'):
            q_dict = question.dict()
        else:
            q_dict = question
            
        required_fields = ["question", "options", "correct_answer", "explanation"]
        
        for field in required_fields:
            if field not in q_dict:
                return False
        
        if len(q_dict.get("options", [])) != 4:
            return False
        
        if q_dict.get("correct_answer") not in q_dict.get("options", []):
            return False
        
        return True

## ì‚°ì¶œë¬¼ 4: ìµœì‹  Gradio ì¸í„°í˜ì´ìŠ¤

### í†µí•© Gradio ì±—ë´‡ (api/gradio_app.py)
```python
import gradio as gr
import requests
import json
from typing import List, Tuple, Optional
import logging

logger = logging.getLogger(__name__)

class SmartLearningChatbot:
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.current_content_id = None
        
    def upload_document(self, file_path: str, title: str = "", category: str = "") -> str:
        """ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(file_path, 'rb') as f:
                files = {'file': f}
                data = {
                    'title': title or "ì—…ë¡œë“œëœ ë¬¸ì„œ",
                    'category': category or "ì¼ë°˜",
                    'content_id': self.current_content_id or 1
                }
                
                response = requests.post(
                    f"{self.api_base_url}/upload-document",
                    files=files,
                    data=data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    self.current_content_id = result.get('content_id')
                    return f" {result.get('message')} ({result.get('chunks_processed')}ê°œ ì²­í¬ ì²˜ë¦¬ë¨)"
                else:
                    return f" ì—…ë¡œë“œ ì‹¤íŒ¨: {response.text}"
                    
        except Exception as e:
            return f" ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def chat_with_langserve(self, message: str, use_rag: bool = True) -> str:
        """LangServe APIë¥¼ í†µí•´ ì±„íŒ…í•©ë‹ˆë‹¤."""
        try:
            endpoint = "/rag/invoke" if use_rag else "/qa/invoke"
            
            payload = {
                "input": {
                    "question": message,
                    "content_id": self.current_content_id
                }
            }
            
            response = requests.post(
                f"{self.api_base_url}{endpoint}",
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("output", {}).get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                confidence = result.get("output", {}).get("confidence", 0.0)
                
                return f"{answer}\n\n ì‹ ë¢°ë„: {confidence:.2f}"
            else:
                return f" API ì˜¤ë¥˜: {response.text}"
                
        except Exception as e:
            return f" ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def generate_quiz(self, difficulty: str = "ë³´í†µ", num_questions: int = 3) -> str:
        """í€´ì¦ˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if not self.current_content_id:
                return " ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            # í€´ì¦ˆ ìƒì„± API í˜¸ì¶œ (ì‹¤ì œë¡œëŠ” ë³„ë„ ì—”ë“œí¬ì¸íŠ¸ í•„ìš”)
            payload = {
                "content_id": self.current_content_id,
                "difficulty": difficulty,
                "num_questions": num_questions
            }
            
            # ì„ì‹œë¡œ ê°„ë‹¨í•œ í€´ì¦ˆ ë°˜í™˜
            return f"""ğŸ“ **í€´ì¦ˆê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!**

**ë¬¸ì œ 1**: ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?
- A) ì˜µì…˜ 1
- B) ì˜µì…˜ 2  
- C) ì˜µì…˜ 3
- D) ì˜µì…˜ 4

*ì •ë‹µ: C) ì˜µì…˜ 3*
*í•´ì„¤: ë¬¸ì„œì—ì„œ ì´ ë¶€ë¶„ì„ ê°•ì¡°í•˜ê³  ìˆìŠµë‹ˆë‹¤.*

ë‚œì´ë„: {difficulty} | ë¬¸ì œ ìˆ˜: {num_questions}ê°œ
"""
            
        except Exception as e:
            return f" í€´ì¦ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
def create_gradio_interface():
    chatbot_instance = SmartLearningChatbot()
    
    with gr.Blocks(
        title=" Smart Learning AI",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        .chat-message {
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
        }
        """
    ) as demo:
        
        gr.Markdown("#  Smart Learning AI Assistant")
        gr.Markdown("AI ê¸°ë°˜ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("###  ë¬¸ì„œ ì—…ë¡œë“œ")
                
                file_input = gr.File(
                    label="ë¬¸ì„œ íŒŒì¼",
                    file_types=[".pdf", ".txt", ".docx"],
                    file_count="single"
                )
                
                with gr.Row():
                    title_input = gr.Textbox(
                        label="ë¬¸ì„œ ì œëª©",
                        placeholder="ë¬¸ì„œ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”"
                    )
                    category_input = gr.Textbox(
                        label="ì¹´í…Œê³ ë¦¬",
                        placeholder="ì¹´í…Œê³ ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
                    )
                
                upload_btn = gr.Button("ğŸ“¤ ì—…ë¡œë“œ", variant="primary")
                upload_status = gr.Textbox(label="ì—…ë¡œë“œ ìƒíƒœ", interactive=False)
                
                gr.Markdown("### âš™ï¸ ì„¤ì •")
                use_rag = gr.Checkbox(label="ê³ ê¸‰ RAG ëª¨ë“œ ì‚¬ìš©", value=True)
                
                gr.Markdown("### ğŸ“ í€´ì¦ˆ ìƒì„±")
                quiz_difficulty = gr.Dropdown(
                    choices=["ì‰¬ì›€", "ë³´í†µ", "ì–´ë ¤ì›€"],
                    value="ë³´í†µ",
                    label="ë‚œì´ë„"
                )
                quiz_count = gr.Slider(
                    minimum=1, maximum=10, value=3, step=1,
                    label="ë¬¸ì œ ìˆ˜"
                )
                quiz_btn = gr.Button(" í€´ì¦ˆ ìƒì„±")
            
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    label="ğŸ’¬ ëŒ€í™”",
                    height=500,
                    show_label=True
                )
                
                msg_input = gr.Textbox(
                    label="ë©”ì‹œì§€ ì…ë ¥",
                    placeholder="í•™ìŠµ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”...",
                    lines=2
                )
                
                with gr.Row():
                    send_btn = gr.Button("ğŸ“¤ ì „ì†¡", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”")
        
        # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
        gr.Markdown("###  ì§ˆë¬¸ ì˜ˆì‹œ")
        example_questions = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "í•µì‹¬ ê°œë…ë“¤ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", 
            "ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        with gr.Row():
            for question in example_questions:
                gr.Button(question, size="sm").click(
                    lambda q=question: q,
                    outputs=msg_input
                )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë“¤
        def handle_upload(file, title, category):
            if file is not None:
                return chatbot_instance.upload_document(file.name, title, category)
            return " íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        
        def handle_chat(message, history, use_rag_mode):
            if not message.strip():
                return history, ""
            
            # ë´‡ ì‘ë‹µ ìƒì„±
            bot_response = chatbot_instance.chat_with_langserve(message, use_rag_mode)
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            history.append((message, bot_response))
            return history, ""
        
        def handle_quiz_generation(difficulty, count):
            return chatbot_instance.generate_quiz(difficulty, count)
        
        # ì´ë²¤íŠ¸ ì—°ê²°
        upload_btn.click(
            handle_upload,
            inputs=[file_input, title_input, category_input],
            outputs=upload_status
        )
        
        send_btn.click(
            handle_chat,
            inputs=[msg_input, chatbot, use_rag],
            outputs=[chatbot, msg_input]
        )
        
        msg_input.submit(
            handle_chat,
            inputs=[msg_input, chatbot, use_rag],
            outputs=[chatbot, msg_input]
        )
        
        quiz_btn.click(
            handle_quiz_generation,
            inputs=[quiz_difficulty, quiz_count],
            outputs=chatbot
        )
        
        clear_btn.click(
            lambda: [],
            outputs=chatbot
        )
    
    return demo

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
```

## ì‚°ì¶œë¬¼ 5: ìµœì‹  requirements.txt

```txt
# í•µì‹¬ LangChain íŒ¨í‚¤ì§€
langchain==0.1.0
langchain-core==0.1.0
langchain-community==0.0.20
langchain-openai==0.0.5
langchain-ollama==0.0.1

# LangGraphì™€ LangServe
langgraph==0.0.30
langserve[all]==0.0.30

# FastAPI ë° ì›¹ ì„œë²„
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.0

# ë¬¸ì„œ ì²˜ë¦¬
pypdf==4.0.1
python-docx==1.1.0
python-multipart==0.0.6

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
faiss-cpu==1.7.4

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.0
tiktoken==0.5.2
numpy==1.24.3
pandas==2.1.4

# Gradio ì¸í„°í˜ì´ìŠ¤
gradio==4.15.0

# ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
python-json-logger==2.0.7
prometheus-client==0.19.0

# í…ŒìŠ¤íŒ…
pytest==7.4.4
pytest-asyncio==0.23.3
httpx==0.26.0
```

## ì‚°ì¶œë¬¼ 6: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

### ìµœì‹  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (tests/test_integration.py)
```python
import pytest
import asyncio
import httpx
from langchain_core.documents import Document

class TestModernAIModule:
    
    @pytest.mark.asyncio
    async def test_langserve_endpoints(self):
        """LangServe ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""
        async with httpx.AsyncClient(base_url="http://localhost:8000") as client:
            
            # ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
            chat_payload = {
                "input": {"question": "ì•ˆë…•í•˜ì„¸ìš”"}
            }
            response = await client.post("/chat/invoke", json=chat_payload)
            assert response.status_code == 200
            
            # QA ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸  
            qa_payload = {
                "input": {
                    "question": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤",
                    "content_id": 1
                }
            }
            response = await client.post("/qa/invoke", json=qa_payload)
            assert response.status_code == 200
            
            # RAG ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
            response = await client.post("/rag/invoke", json=qa_payload)
            assert response.status_code == 200

    def test_modern_document_processing(self):
        """ìµœì‹  ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        from src.core.document_processor import DocumentProcessor
        
        processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìƒì„±
        test_content = "ì´ê²ƒì€ ìµœì‹  LangChainì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤. " * 100
        with open("test_modern_doc.txt", "w", encoding="utf-8") as f:
            f.write(test_content)
        
        # ë¬¸ì„œ ì²˜ë¦¬
        chunks = processor.process_file("test_modern_doc.txt", {"test": True})
        
        assert len(chunks) > 0
        assert all(len(chunk.page_content) <= 500 for chunk in chunks)
        assert all(chunk.metadata.get("test") is True for chunk in chunks)
        
        # ì •ë¦¬
        import os
        os.remove("test_modern_doc.txt")

    def test_modern_vector_store(self):
        """ìµœì‹  ë²¡í„° ìŠ¤í† ì–´ í…ŒìŠ¤íŠ¸"""
        from src.core.vector_store import VectorStoreManager
        
        vector_manager = VectorStoreManager(
            embedding_provider="openai",
            model_name="text-embedding-3-small"
        )
        
        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ
        docs = [
            Document(
                page_content="LangChain 0.1.0ì€ ìµœì‹  AI í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.", 
                metadata={"id": 1, "version": "0.1.0"}
            ),
            Document(
                page_content="LangServeë¥¼ ì‚¬ìš©í•˜ë©´ ì‰½ê²Œ APIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", 
                metadata={"id": 2, "version": "0.1.0"}
            )
        ]
        
        # ë¬¸ì„œ ì¶”ê°€
        vector_manager.add_documents(docs, content_id=1)
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        results = vector_manager.similarity_search("LangChain ìµœì‹ ", k=1)
        assert len(results) > 0
        assert "LangChain" in results[0].page_content

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

## ì£¼ìš” ì—…ë°ì´íŠ¸ í¬ì¸íŠ¸

### 1. **LangChain 0.1.0+ í˜¸í™˜ì„±**
- ìƒˆë¡œìš´ ì„í¬íŠ¸ êµ¬ì¡° ì ìš©
- `create_retrieval_chain`, `create_stuff_documents_chain` ì‚¬ìš©
- ìµœì‹  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¬¸ë²•

### 2. **LangServe í†µí•©**
- FastAPI ëŒ€ì‹  LangServeì˜ `add_routes` ì‚¬ìš©
- Runnable ê¸°ë°˜ ì²´ì¸ ì„¤ê³„
- ìë™ API ë¬¸ì„œí™” ì§€ì›

### 3. **LangGraph ìµœì‹  ë¬¸ë²•**
- `START`, `END` ëª…ì‹œì  ì‚¬ìš©
- ê°œì„ ëœ ìƒíƒœ ê´€ë¦¬
- ë” ì§ê´€ì ì¸ ê·¸ë˜í”„ êµ¬ì¡°

### 4. **ì„±ëŠ¥ ìµœì í™”**
- ë²¡í„° ìŠ¤í† ì–´ ìºì‹± ì „ëµ
- ë¹„ë™ê¸° ì²˜ë¦¬ ê°•í™”  
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 

ì´ì œ ìµœì‹  LangChain/LangGraph ë²„ì „ì— ë§ì¶° ì™„ì „íˆ ì—…ë°ì´íŠ¸ëœ ê°€ì´ë“œì…ë‹ˆë‹¤! 