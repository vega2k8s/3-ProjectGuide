# Day 5: ì»¨í…Œì´ë„ˆí™” ë° CI/CD (ìµœì‹  LangServe í†µí•©)

## ì‚°ì¶œë¬¼ 1: ìµœì‹  Docker ì„¤ì • íŒŒì¼ë“¤

### AI Module Dockerfile (LangServe ìµœì í™”)
```dockerfile
# ai-module/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ë³µì‚¬ ë° ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬
COPY src/ ./src/
COPY data/ ./data/
COPY .env ./

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p data/vector_store uploaded_documents logs

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ìš©ì ìƒì„±
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# í¬íŠ¸ ë…¸ì¶œ (LangServe ê¸°ë³¸ í¬íŠ¸)
EXPOSE 8000

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV LANGCHAIN_TRACING_V2=true
ENV LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
ENV LANGCHAIN_PROJECT=smart-learning-production

# í—¬ìŠ¤ì²´í¬ (LangServe ì—”ë“œí¬ì¸íŠ¸)
HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# LangServe ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

### Backend Dockerfile (AI í†µí•© ìµœì í™”)
```dockerfile
# backend/Dockerfile
FROM maven:3.9-openjdk-17-slim AS build

WORKDIR /app

# Maven ì„¤ì • ë° ì˜ì¡´ì„± ë³µì‚¬
COPY .mvn .mvn
COPY mvnw pom.xml ./

# ì˜ì¡´ì„± ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìµœì í™”)
RUN ./mvnw dependency:go-offline -B

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬ ë° ë¹Œë“œ
COPY src ./src
RUN ./mvnw clean package -DskipTests -B

# ì‹¤í–‰ ìŠ¤í…Œì´ì§€
FROM openjdk:17-jdk-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
RUN apt-get update && apt-get install -y \
    curl \
    netcat-traditional \
    && rm -rf /var/lib/apt/lists/*

# JAR íŒŒì¼ ë³µì‚¬
COPY --from=build /app/target/*.jar app.jar

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
RUN mkdir -p uploads logs

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ìš©ì ìƒì„±
RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN chown -R appuser:appuser /app
USER appuser

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8080

# JVM ìµœì í™” ì„¤ì •
ENV JAVA_OPTS="-Xmx1g -Xms512m -XX:+UseG1GC -XX:+UseContainerSupport"

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

# ëŒ€ê¸° ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
RUN echo '#!/bin/bash\n\
echo "Waiting for AI service to be ready..."\n\
while ! nc -z ai-service 8000; do\n\
  sleep 2\n\
done\n\
echo "AI service is ready!"\n\
exec java $JAVA_OPTS -jar app.jar' > wait-and-start.sh && \
chmod +x wait-and-start.sh

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
ENTRYPOINT ["./wait-and-start.sh"]
```

### Frontend Dockerfile (ìµœì‹  React + AI í†µí•©)
```dockerfile
# frontend/Dockerfile
# Build stage
FROM node:18-alpine AS build

WORKDIR /app

# package.jsonê³¼ lock íŒŒì¼ ë³µì‚¬ (ìºì‹œ ìµœì í™”)
COPY package.json package-lock.json ./

# ì˜ì¡´ì„± ì„¤ì¹˜
RUN npm ci --only=production --silent

# ì†ŒìŠ¤ ì½”ë“œ ë³µì‚¬ ë° ë¹Œë“œ
COPY . .

# í™˜ê²½ë³„ ë¹Œë“œ ì„¤ì •
ARG REACT_APP_API_URL
ARG REACT_APP_AI_URL
ARG NODE_ENV=production

ENV REACT_APP_API_URL=$REACT_APP_API_URL
ENV REACT_APP_AI_URL=$REACT_APP_AI_URL
ENV NODE_ENV=$NODE_ENV

RUN npm run build

# Production stage
FROM nginx:alpine

# ë³´ì•ˆ ì—…ë°ì´íŠ¸
RUN apk update && apk upgrade && apk add --no-cache curl

# Nginx ì„¤ì • ë³µì‚¬
COPY nginx.conf /etc/nginx/nginx.conf

# ë¹Œë“œëœ íŒŒì¼ ë³µì‚¬
COPY --from=build /app/build /usr/share/nginx/html

# ê¶Œí•œ ì„¤ì •
RUN chown -R nginx:nginx /usr/share/nginx/html && \
    chmod -R 755 /usr/share/nginx/html

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 80

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
    CMD curl -f http://localhost/health || exit 1

# Nginx ì‹¤í–‰
CMD ["nginx", "-g", "daemon off;"]
```

### ìµœì‹  Nginx ì„¤ì • (LangServe ë¼ìš°íŒ… í¬í•¨)
```nginx
# frontend/nginx.conf
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    
    # ë¡œê¹… ì„¤ì •
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    # ì—…ìŠ¤íŠ¸ë¦¼ ì •ì˜
    upstream backend {
        server backend:8080 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    upstream ai-service {
        server ai-service:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
    
    # ì„±ëŠ¥ ìµœì í™”
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50m;
    
    # ë²„í¼ í¬ê¸° ìµœì í™”
    proxy_buffer_size 128k;
    proxy_buffers 4 256k;
    proxy_busy_buffers_size 256k;
    
    # GZIP ì••ì¶•
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json
        application/atom+xml
        image/svg+xml;
    
    server {
        listen 80;
        server_name localhost;
        root /usr/share/nginx/html;
        index index.html;
        
        # í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # SPA ë¼ìš°íŒ… ì§€ì›
        location / {
            try_files $uri $uri/ /index.html;
            
            # ìºì‹œ ì„¤ì •
            add_header Cache-Control "no-cache, no-store, must-revalidate";
            add_header Pragma "no-cache";
            add_header Expires "0";
        }
        
        # ì •ì  íŒŒì¼ ìºì‹±
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
            add_header X-Content-Type-Options "nosniff";
        }
        
        # Backend API í”„ë¡ì‹œ
        location /api/ {
            proxy_pass http://backend/api/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # AI ì„œë¹„ìŠ¤ í”„ë¡ì‹œ (LangServe)
        location /ai/ {
            rewrite ^/ai/(.*) /$1 break;
            proxy_pass http://ai-service;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
            proxy_buffering off;
            proxy_cache off;
            proxy_read_timeout 300s;
            proxy_connect_timeout 75s;
            
            # Server-Sent Events ì§€ì›
            proxy_set_header Connection '';
            chunked_transfer_encoding off;
        }
        
        # LangServe Playground ì ‘ê·¼
        location /langserve/ {
            proxy_pass http://ai-service/;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Playground ìµœì í™”
            proxy_buffering off;
            proxy_cache off;
        }
        
        # WebSocket ì§€ì›
        location /ws/ {
            proxy_pass http://backend/ws/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # ë©”íŠ¸ë¦­ í”„ë¡ì‹œ
        location /metrics {
            proxy_pass http://backend/actuator/metrics;
            proxy_set_header Host $host;
            access_log off;
        }
        
        # ë³´ì•ˆ í—¤ë”
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-XSS-Protection "1; mode=block" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header Referrer-Policy "strict-origin-when-cross-origin" always;
        add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; connect-src 'self' ws: wss:;" always;
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
    }
}
```

## ì‚°ì¶œë¬¼ 2: ìµœì‹  Docker Compose ì„¤ì •

### docker-compose.yml (LangServe í†µí•©)
```yaml
version: '3.8'

services:
  # ë°ì´í„°ë² ì´ìŠ¤
  database:
    image: mariadb:10.11
    container_name: smart-learning-db
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD:-rootpassword}
      MYSQL_DATABASE: ${DB_NAME:-smart_learning_db}
      MYSQL_USER: ${DB_USER:-app_user}
      MYSQL_PASSWORD: ${DB_PASSWORD:-app_password}
      MYSQL_CHARSET: utf8mb4
      MYSQL_COLLATION: utf8mb4_unicode_ci
    ports:
      - "${DB_PORT:-3306}:3306"
    volumes:
      - db_data:/var/lib/mysql
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./database/my.cnf:/etc/mysql/conf.d/my.cnf:ro
    networks:
      - app-network
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${DB_ROOT_PASSWORD:-rootpassword}"]
      timeout: 20s
      retries: 10
      interval: 30s
      start_period: 40s

  # Redis ìºì‹œ
  redis:
    image: redis:7-alpine
    container_name: smart-learning-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis_password}
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      timeout: 3s
      retries: 5
      interval: 30s

  # AI ì„œë¹„ìŠ¤ (LangServe)
  ai-service:
    build:
      context: ./ai-module
      dockerfile: Dockerfile
      args:
        - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    container_name: smart-learning-ai
    restart: unless-stopped
    environment:
      # LangChain ì„¤ì •
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}
      LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-true}
      LANGCHAIN_PROJECT: ${LANGCHAIN_PROJECT:-smart-learning}
      LANGCHAIN_ENDPOINT: https://api.smith.langchain.com
      
      # AI ëª¨ë¸ ì„¤ì •
      MODEL_NAME: ${AI_MODEL_NAME:-gpt-3.5-turbo}
      EMBEDDINGS_MODEL: ${EMBEDDINGS_MODEL:-text-embedding-3-small}
      
      # ì„œë¹„ìŠ¤ ì„¤ì •
      DATABASE_URL: mysql://${DB_USER:-app_user}:${DB_PASSWORD:-app_password}@database:3306/${DB_NAME:-smart_learning_db}
      REDIS_URL: redis://redis:6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
      LOG_LEVEL: ${AI_LOG_LEVEL:-INFO}
      
      # ì„±ëŠ¥ ì„¤ì •
      WORKERS: ${AI_WORKERS:-2}
      MAX_CONCURRENT_REQUESTS: ${AI_MAX_CONCURRENT:-10}
    ports:
      - "${AI_PORT:-8000}:8000"
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - ai_data:/app/data
      - ai_uploads:/app/uploaded_documents
      - ai_logs:/app/logs
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      timeout: 30s
      retries: 5
      interval: 30s
      start_period: 120s

  # Backend API
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: smart-learning-backend
    restart: unless-stopped
    environment:
      # Spring ì„¤ì •
      SPRING_PROFILES_ACTIVE: ${SPRING_PROFILES_ACTIVE:-docker}
      
      # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
      DB_HOST: database
      DB_PORT: 3306
      DB_NAME: ${DB_NAME:-smart_learning_db}
      DB_USER: ${DB_USER:-app_user}
      DB_PASSWORD: ${DB_PASSWORD:-app_password}
      
      # Redis ì„¤ì •
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis_password}
      
      # ë³´ì•ˆ ì„¤ì •
      JWT_SECRET: ${JWT_SECRET:-mySecretKey123!@#$%^&*()_+}
      
      # AI ì„œë¹„ìŠ¤ ì„¤ì •
      AI_SERVICE_URL: http://ai-service:8000
      AI_SERVICE_TIMEOUT: 300
      
      # íŒŒì¼ ì—…ë¡œë“œ ì„¤ì •
      FILE_UPLOAD_MAX_SIZE: 50MB
      FILE_UPLOAD_DIR: /app/uploads
      
      # ëª¨ë‹ˆí„°ë§ ì„¤ì •
      MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: health,info,metrics,prometheus
      MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS: always
    ports:
      - "${BACKEND_PORT:-8080}:8080"
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy
      ai-service:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - backend_uploads:/app/uploads
      - backend_logs:/app/logs
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      timeout: 10s
      retries: 5
      interval: 30s
      start_period: 90s

  # Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - REACT_APP_API_URL=${REACT_APP_API_URL:-http://localhost:8080/api}
        - REACT_APP_AI_URL=${REACT_APP_AI_URL:-http://localhost:8000}
        - NODE_ENV=${NODE_ENV:-production}
    container_name: smart-learning-frontend
    restart: unless-stopped
    ports:
      - "${FRONTEND_PORT:-80}:80"
    depends_on:
      - backend
      - ai-service
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      timeout: 5s
      retries: 3
      interval: 30s
      start_period: 30s

  # ëª¨ë‹ˆí„°ë§ - Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: smart-learning-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
      - prometheus_data:/prometheus
    networks:
      - app-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring

  # ëª¨ë‹ˆí„°ë§ - Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: smart-learning-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - app-network
    depends_on:
      - prometheus
    profiles:
      - monitoring

  # ë¡œê·¸ ìˆ˜ì§‘ - Loki
  loki:
    image: grafana/loki:latest
    container_name: smart-learning-loki
    restart: unless-stopped
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - app-network
    profiles:
      - monitoring

  # ë¡œê·¸ ìˆ˜ì§‘ ì—ì´ì „íŠ¸ - Promtail
  promtail:
    image: grafana/promtail:latest
    container_name: smart-learning-promtail
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yml:ro
      - ai_logs:/var/log/ai:ro
      - backend_logs:/var/log/backend:ro
    networks:
      - app-network
    depends_on:
      - loki
    profiles:
      - monitoring

volumes:
  db_data:
    driver: local
  redis_data:
    driver: local
  backend_uploads:
    driver: local
  backend_logs:
    driver: local
  ai_data:
    driver: local
  ai_uploads:
    driver: local
  ai_logs:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local

networks:
  app-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
    driver_opts:
      com.docker.network.bridge.name: smart-learning-net
```

### docker-compose.dev.yml (ê°œë°œ í™˜ê²½)
```yaml
version: '3.8'

services:
  ai-service:
    build:
      context: ./ai-module
      dockerfile: Dockerfile
      target: dev
    volumes:
      - ./ai-module/src:/app/src:ro
      - ./ai-module/tests:/app/tests:ro
    environment:
      DEBUG: true
      LOG_LEVEL: DEBUG
      LANGCHAIN_TRACING_V2: true
      RELOAD: true
    command: ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    ports:
      - "8000:8000"
      - "8001:8001"  # ë©”íŠ¸ë¦­ í¬íŠ¸

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: dev
    volumes:
      - ./backend/src:/app/src:ro
      - ./backend/target:/app/target
    environment:
      SPRING_PROFILES_ACTIVE: dev
      SPRING_DEVTOOLS_RESTART_ENABLED: true
      SPRING_DEVTOOLS_LIVERELOAD_ENABLED: true
    ports:
      - "8080:8080"
      - "35729:35729"  # LiveReload

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    volumes:
      - ./frontend/src:/app/src:ro
      - ./frontend/public:/app/public:ro
    environment:
      NODE_ENV: development
      CHOKIDAR_USEPOLLING: true
      FAST_REFRESH: true
    command: ["npm", "start"]
    ports:
      - "3000:3000"

  # ê°œë°œìš© ë„êµ¬ë“¤
  mailhog:
    image: mailhog/mailhog:latest
    container_name: smart-learning-mailhog
    ports:
      - "1025:1025"  # SMTP
      - "8025:8025"  # Web UI
    networks:
      - app-network

  adminer:
    image: adminer:latest
    container_name: smart-learning-adminer
    ports:
      - "8081:8080"
    networks:
      - app-network
    depends_on:
      - database
```

## ì‚°ì¶œë¬¼ 3: GitHub Actions CI/CD íŒŒì´í”„ë¼ì¸ (ìµœì‹ í™”)

### .github/workflows/ci-cd.yml
```yaml
name: Smart Learning CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository }}

jobs:
  # ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬ ë° í…ŒìŠ¤íŠ¸
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    strategy:
      matrix:
        component: [backend, frontend, ai-module]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Backend Testing
      if: matrix.component == 'backend'
      uses: actions/setup-java@v4
      with:
        java-version: '17'
        distribution: 'temurin'
        cache: maven

    - name: Setup Frontend Testing
      if: matrix.component == 'frontend'
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Setup AI Module Testing
      if: matrix.component == 'ai-module'
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: ai-module/requirements.txt

    # Backend í…ŒìŠ¤íŠ¸
    - name: Run Backend Tests
      if: matrix.component == 'backend'
      run: |
        cd backend
        ./mvnw clean verify -Dspring.profiles.active=test
        ./mvnw jacoco:report

    - name: Upload Backend Coverage
      if: matrix.component == 'backend'
      uses: codecov/codecov-action@v4
      with:
        files: backend/target/site/jacoco/jacoco.xml
        flags: backend
        token: ${{ secrets.CODECOV_TOKEN }}

    # Frontend í…ŒìŠ¤íŠ¸
    - name: Install Frontend Dependencies
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm ci --prefer-offline --no-audit

    - name: Run Frontend Lint
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm run lint:check

    - name: Run Frontend Tests
      if: matrix.component == 'frontend'
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false --testResultsProcessor=jest-sonar-reporter
      env:
        CI: true

    - name: Upload Frontend Coverage
      if: matrix.component == 'frontend'
      uses: codecov/codecov-action@v4
      with:
        files: frontend/coverage/lcov.info
        flags: frontend
        token: ${{ secrets.CODECOV_TOKEN }}

    # AI Module í…ŒìŠ¤íŠ¸
    - name: Install AI Dependencies
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run AI Module Lint
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        black --check src/
        flake8 src/
        mypy src/

    - name: Run AI Module Tests
      if: matrix.component == 'ai-module'
      run: |
        cd ai-module
        python -m pytest tests/ -v --cov=src --cov-report=xml --cov-report=html
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Upload AI Coverage
      if: matrix.component == 'ai-module'
      uses: codecov/codecov-action@v4
```      
## ì‚°ì¶œë¬¼ 4: ìµœì‹  ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

### deploy-k8s.sh (Kubernetes ë°°í¬)
```bash
#!/bin/bash
set -euo pipefail

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# ë¡œê¹… í•¨ìˆ˜
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[STEP]${NC} $1"; }

# ê¸°ë³¸ ì„¤ì •
ENVIRONMENT=""
IMAGE_TAG="latest"
DEPLOYMENT_STRATEGY="rolling"
OPENAI_KEY=""
LANGCHAIN_KEY=""
NAMESPACE="smart-learning"
CHART_PATH="./helm/smart-learning"

# ë„ì›€ë§
show_help() {
    cat << EOF
Kubernetes ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•: $0 <í™˜ê²½> [ì˜µì…˜]

í™˜ê²½:
    staging     ìŠ¤í…Œì´ì§• í™˜ê²½
    production  í”„ë¡œë•ì…˜ í™˜ê²½

ì˜µì…˜:
    --image-tag TAG         Docker ì´ë¯¸ì§€ íƒœê·¸ (ê¸°ë³¸ê°’: latest)
    --strategy STRATEGY     ë°°í¬ ì „ëµ (rolling|blue-green|canary)
    --openai-key KEY        OpenAI API í‚¤
    --langchain-key KEY     LangChain API í‚¤
    --namespace NS          Kubernetes ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    --dry-run              ì‹¤ì œ ë°°í¬ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰
    -h, --help             ì´ ë„ì›€ë§ í‘œì‹œ

ì˜ˆì‹œ:
    $0 staging --image-tag v1.2.3 --strategy rolling
    $0 production --image-tag stable --strategy blue-green

EOF
}

# íŒŒë¼ë¯¸í„° íŒŒì‹±
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            staging|production)
                ENVIRONMENT=$1
                shift
                ;;
            --image-tag)
                IMAGE_TAG="$2"
                shift 2
                ;;
            --strategy)
                DEPLOYMENT_STRATEGY="$2"
                shift 2
                ;;
            --openai-key)
                OPENAI_KEY="$2"
                shift 2
                ;;
            --langchain-key)
                LANGCHAIN_KEY="$2"
                shift 2
                ;;
            --namespace)
                NAMESPACE="$2"
                shift 2
                ;;
            --dry-run)
                DRY_RUN=true
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option $1"
                show_help
                exit 1
                ;;
        esac
    done

    if [[ -z "$ENVIRONMENT" ]]; then
        log_error "í™˜ê²½ì„ ì§€ì •í•´ì£¼ì„¸ìš” (staging ë˜ëŠ” production)"
        show_help
        exit 1
    fi
}

# ì‚¬ì „ ì¡°ê±´ í™•ì¸
check_prerequisites() {
    log_step "ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì¤‘..."

    # í•„ìˆ˜ ë„êµ¬ í™•ì¸
    local tools=("kubectl" "helm")
    for tool in "${tools[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            log_error "$toolì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            exit 1
        fi
    done

    # Kubernetes ì—°ê²° í™•ì¸
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Kubernetes í´ëŸ¬ìŠ¤í„°ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        exit 1
    fi

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸/ìƒì„±
    if ! kubectl get namespace "$NAMESPACE" &> /dev/null; then
        log_info "ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '$NAMESPACE' ìƒì„± ì¤‘..."
        kubectl create namespace "$NAMESPACE"
    fi

    # Helm ì°¨íŠ¸ í™•ì¸
    if [[ ! -d "$CHART_PATH" ]]; then
        log_error "Helm ì°¨íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $CHART_PATH"
        exit 1
    fi

    log_info "ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì™„ë£Œ"
}

# ì‹œí¬ë¦¿ ê´€ë¦¬
manage_secrets() {
    log_step "ì‹œí¬ë¦¿ ê´€ë¦¬ ì¤‘..."

    # AI ì„œë¹„ìŠ¤ ì‹œí¬ë¦¿
    if [[ -n "$OPENAI_KEY" && -n "$LANGCHAIN_KEY" ]]; then
        kubectl create secret generic ai-secrets \
            --namespace="$NAMESPACE" \
            --from-literal=openai-api-key="$OPENAI_KEY" \
            --from-literal=langchain-api-key="$LANGCHAIN_KEY" \
            --dry-run=client -o yaml | kubectl apply -f -
        log_info "AI ì„œë¹„ìŠ¤ ì‹œí¬ë¦¿ ì—…ë°ì´íŠ¸ ì™„ë£Œ"
    fi

    # í™˜ê²½ë³„ ì‹œí¬ë¦¿ ë¡œë“œ
    local secret_file="./k8s/secrets/${ENVIRONMENT}-secrets.yaml"
    if [[ -f "$secret_file" ]]; then
        kubectl apply -f "$secret_file" -n "$NAMESPACE"
        log_info "í™˜ê²½ë³„ ì‹œí¬ë¦¿ ì ìš© ì™„ë£Œ"
    fi
}

# Blue-Green ë°°í¬
deploy_blue_green() {
    log_step "Blue-Green ë°°í¬ ì‹œì‘..."
    
    local current_version=$(kubectl get deployment backend -n "$NAMESPACE" -o jsonpath='{.metadata.labels.version}' 2>/dev/null || echo "blue")
    local new_version="green"
    
    if [[ "$current_version" == "green" ]]; then
        new_version="blue"
    fi

    log_info "í˜„ì¬ ë²„ì „: $current_version, ìƒˆ ë²„ì „: $new_version"

    # ìƒˆ ë²„ì „ ë°°í¬
    helm upgrade --install "smart-learning-$new_version" "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="$new_version" \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=10m

    # í—¬ìŠ¤ì²´í¬
    if ! run_health_checks "$new_version"; then
        log_error "ìƒˆ ë²„ì „ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
        rollback_deployment "$new_version"
        return 1
    fi

    # íŠ¸ë˜í”½ ì „í™˜
    log_info "íŠ¸ë˜í”½ì„ ìƒˆ ë²„ì „ìœ¼ë¡œ ì „í™˜ ì¤‘..."
    kubectl patch service frontend -n "$NAMESPACE" \
        -p '{"spec":{"selector":{"version":"'$new_version'"}}}'

    # ì´ì „ ë²„ì „ ì •ë¦¬ (5ë¶„ í›„)
    log_info "5ë¶„ í›„ ì´ì „ ë²„ì „ì„ ì •ë¦¬í•©ë‹ˆë‹¤..."
    sleep 300
    helm uninstall "smart-learning-$current_version" -n "$NAMESPACE" || true

    log_info "Blue-Green ë°°í¬ ì™„ë£Œ"
}

# ì¹´ë‚˜ë¦¬ ë°°í¬
deploy_canary() {
    log_step "ì¹´ë‚˜ë¦¬ ë°°í¬ ì‹œì‘..."

    # í˜„ì¬ ë²„ì „ì„ stableë¡œ í‘œì‹œ
    kubectl patch deployment backend -n "$NAMESPACE" \
        -p '{"metadata":{"labels":{"version":"stable"}}}'

    # ì¹´ë‚˜ë¦¬ ë²„ì „ ë°°í¬ (10% íŠ¸ë˜í”½)
    helm upgrade --install smart-learning-canary "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="canary" \
        --set global.replica.count=1 \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=10m

    # íŠ¸ë˜í”½ ë¶„í•  ì„¤ì • (Istio ì‚¬ìš© ê°€ì •)
    kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: smart-learning-vs
  namespace: $NAMESPACE
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: frontend
        subset: canary
  - route:
    - destination:
        host: frontend
        subset: stable
      weight: 90
    - destination:
        host: frontend
        subset: canary
      weight: 10
EOF

    log_info "ì¹´ë‚˜ë¦¬ ë°°í¬ ì™„ë£Œ (10% íŠ¸ë˜í”½)"

    # 30ë¶„ ëŒ€ê¸° í›„ ì „ì²´ íŠ¸ë˜í”½ ì „í™˜
    log_info "30ë¶„ í›„ ì „ì²´ íŠ¸ë˜í”½ì„ ìƒˆ ë²„ì „ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤..."
    sleep 1800

    if run_health_checks "canary"; then
        # ì „ì²´ íŠ¸ë˜í”½ì„ ì¹´ë‚˜ë¦¬ë¡œ ì „í™˜
        kubectl patch virtualservice smart-learning-vs -n "$NAMESPACE" \
            --type='json' \
            -p='[{"op": "replace", "path": "/spec/http/1/route/0/weight", "value": 0}]'
        kubectl patch virtualservice smart-learning-vs -n "$NAMESPACE" \
            --type='json' \
            -p='[{"op": "replace", "path": "/spec/http/1/route/1/weight", "value": 100}]'
        
        log_info "ì¹´ë‚˜ë¦¬ ë°°í¬ ì„±ê³µ"
    else
        log_error "ì¹´ë‚˜ë¦¬ ë²„ì „ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨, ë¡¤ë°± ì¤‘..."
        rollback_deployment "canary"
    fi
}

# ë¡¤ë§ ë°°í¬
deploy_rolling() {
    log_step "ë¡¤ë§ ë°°í¬ ì‹œì‘..."

    helm upgrade --install smart-learning "$CHART_PATH" \
        --namespace "$NAMESPACE" \
        --set-string global.imageTag="$IMAGE_TAG" \
        --set global.version="stable" \
        --set-file global.environment="./k8s/configs/${ENVIRONMENT}.yaml" \
        --wait --timeout=15m

    log_info "ë¡¤ë§ ë°°í¬ ì™„ë£Œ"
}

# í—¬ìŠ¤ì²´í¬
run_health_checks() {
    local version=${1:-"stable"}
    log_step "í—¬ìŠ¤ì²´í¬ ì‹¤í–‰ ì¤‘... (ë²„ì „: $version)"

    local max_attempts=30
    local attempt=0

    while [[ $attempt -lt $max_attempts ]]; do
        attempt=$((attempt + 1))

        # ëª¨ë“  í¬ë“œê°€ Ready ìƒíƒœì¸ì§€ í™•ì¸
        local ready_pods=$(kubectl get pods -n "$NAMESPACE" \
            -l version="$version" \
            -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | \
            grep -c "True" || echo "0")
        
        local total_pods=$(kubectl get pods -n "$NAMESPACE" \
            -l version="$version" \
            --no-headers | wc -l)

        if [[ "$ready_pods" -eq "$total_pods" && "$total_pods" -gt 0 ]]; then
            log_info "ëª¨ë“  í¬ë“œê°€ Ready ìƒíƒœì…ë‹ˆë‹¤."
            
            # ì• í”Œë¦¬ì¼€ì´ì…˜ í—¬ìŠ¤ì²´í¬
            local service_url=$(kubectl get service frontend -n "$NAMESPACE" \
                -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "localhost")
            
            if curl -f -s "http://$service_url/health" > /dev/null; then
                log_info "ì• í”Œë¦¬ì¼€ì´ì…˜ í—¬ìŠ¤ì²´í¬ ì„±ê³µ"
                return 0
            fi
        fi

        log_warn "í—¬ìŠ¤ì²´í¬ ì¬ì‹œë„ ($attempt/$max_attempts)..."
        sleep 30
    done

    log_error "í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"
    return 1
}

# ë¡¤ë°±
rollback_deployment() {
    local version=${1:-""}
    log_step "ë°°í¬ ë¡¤ë°± ì¤‘..."

    if [[ -n "$version" ]]; then
        helm rollback "smart-learning-$version" -n "$NAMESPACE"
    else
        helm rollback smart-learning -n "$NAMESPACE"
    fi

    log_info "ë¡¤ë°± ì™„ë£Œ"
}

# ë°°í¬ í›„ ê²€ì¦
post_deployment_verification() {
    log_step "ë°°í¬ í›„ ê²€ì¦ ì¤‘..."

    # ê¸°ë³¸ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
    local api_tests=(
        "/health"
        "/api/actuator/health"
        "/ai/health"
    )

    local service_url=$(kubectl get ingress smart-learning-ingress -n "$NAMESPACE" \
        -o jsonpath='{.spec.rules[0].host}' 2>/dev/null || echo "localhost")

    for endpoint in "${api_tests[@]}"; do
        if curl -f -s "https://$service_url$endpoint" > /dev/null; then
            log_info "âœ“ $endpoint ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ"
        else
            log_warn "âœ— $endpoint ì—”ë“œí¬ì¸íŠ¸ ì‹¤íŒ¨"
        fi
    done

    # ë©”íŠ¸ë¦­ í™•ì¸
    if kubectl get pods -n "$NAMESPACE" -l app=prometheus &> /dev/null; then
        log_info "ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ í™•ì¸ ì¤‘..."
        # Prometheus ë©”íŠ¸ë¦­ í™•ì¸ ë¡œì§
    fi

    log_info "ë°°í¬ í›„ ê²€ì¦ ì™„ë£Œ"
}

# ë©”ì¸ í•¨ìˆ˜
main() {
    log_info "Smart Learning Kubernetes ë°°í¬ ì‹œì‘"
    
    parse_args "$@"
    
    log_info "í™˜ê²½: $ENVIRONMENT"
    log_info "ì´ë¯¸ì§€ íƒœê·¸: $IMAGE_TAG" 
    log_info "ë°°í¬ ì „ëµ: $DEPLOYMENT_STRATEGY"
    log_info "ë„¤ì„ìŠ¤í˜ì´ìŠ¤: $NAMESPACE"

    if [[ "${DRY_RUN:-false}" == "true" ]]; then
        log_warn "DRY RUN ëª¨ë“œ - ì‹¤ì œ ë°°í¬ëŠ” ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        exit 0
    fi

    check_prerequisites
    manage_secrets

    # ë°°í¬ ì „ëµì— ë”°ë¥¸ ì‹¤í–‰
    case "$DEPLOYMENT_STRATEGY" in
        "blue-green")
            deploy_blue_green
            ;;
        "canary")
            deploy_canary
            ;;
        "rolling"|*)
            deploy_rolling
            ;;
    esac

    run_health_checks
    post_deployment_verification

    log_info "ë°°í¬ ì™„ë£Œ! ğŸ‰"
    kubectl get pods -n "$NAMESPACE"
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

### docker-deploy.sh (Docker Compose ë°°í¬)
```bash
#!/bin/bash
set -euo pipefail

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# ë¡œê¹… í•¨ìˆ˜
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[STEP]${NC} $1"; }

# ê¸°ë³¸ ì„¤ì •
ENVIRONMENT="production"
COMPOSE_PROJECT_NAME="smart-learning"
BACKUP_ENABLED=true
MONITORING_ENABLED=false
BUILD_IMAGES=false
PULL_IMAGES=true

# ë„ì›€ë§
show_help() {
    cat << EOF
Docker Compose ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•: $0 [ì˜µì…˜]

ì˜µì…˜:
    -e, --environment ENV   ë°°í¬ í™˜ê²½ (dev|staging|production)
    -b, --build            ë¡œì»¬ì—ì„œ ì´ë¯¸ì§€ ë¹Œë“œ
    -p, --pull             ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì´ë¯¸ì§€ í’€ (ê¸°ë³¸ê°’)
    -m, --monitoring       ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ í¬í•¨
    --no-backup           ë°±ì—… ê±´ë„ˆë›°ê¸°
    --cleanup             ê¸°ì¡´ ì»¨í…Œì´ë„ˆ ì •ë¦¬ í›„ ë°°í¬
    -h, --help            ì´ ë„ì›€ë§ í‘œì‹œ

ì˜ˆì‹œ:
    $0 -e staging -m
    $0 --build --monitoring
    $0 --cleanup -e production

EOF
}

# íŒŒë¼ë¯¸í„° íŒŒì‹±
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -e|--environment)
                ENVIRONMENT="$2"
                shift 2
                ;;
            -b|--build)
                BUILD_IMAGES=true
                PULL_IMAGES=false
                shift
                ;;
            -p|--pull)
                PULL_IMAGES=true
                BUILD_IMAGES=false
                shift
                ;;
            -m|--monitoring)
                MONITORING_ENABLED=true
                shift
                ;;
            --no-backup)
                BACKUP_ENABLED=false
                shift
                ;;
            --cleanup)
                CLEANUP_ENABLED=true
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                log_error "Unknown option $1"
                show_help
                exit 1
                ;;
        esac
    done
}

# í™˜ê²½ ì„¤ì •
setup_environment() {
    log_step "í™˜ê²½ ì„¤ì • ì¤‘..."

    # í™˜ê²½ë³„ ì„¤ì • íŒŒì¼ ì„ íƒ
    case "$ENVIRONMENT" in
        "dev"|"development")
            COMPOSE_FILE="docker-compose.dev.yml"
            ENV_FILE=".env.development"
            ;;
        "staging")
            COMPOSE_FILE="docker-compose.staging.yml"
            ENV_FILE=".env.staging"
            ;;
        "production")
            COMPOSE_FILE="docker-compose.yml"
            ENV_FILE=".env.production"
            ;;
        *)
            log_error "ì§€ì›í•˜ì§€ ì•ŠëŠ” í™˜ê²½: $ENVIRONMENT"
            exit 1
            ;;
    esac

    # í™˜ê²½ íŒŒì¼ í™•ì¸
    if [[ ! -f "$ENV_FILE" ]]; then
        log_error "í™˜ê²½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $ENV_FILE"
        exit 1
    fi

    # Compose íŒŒì¼ í™•ì¸
    if [[ ! -f "$COMPOSE_FILE" ]]; then
        log_error "Compose íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $COMPOSE_FILE"
        exit 1
    fi

    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    if [[ "$MONITORING_ENABLED" == "true" ]]; then
        COMPOSE_PROFILES="--profile monitoring"
    else
        COMPOSE_PROFILES=""
    fi

    log_info "í™˜ê²½: $ENVIRONMENT"
    log_info "Compose íŒŒì¼: $COMPOSE_FILE"
    log_info "í™˜ê²½ íŒŒì¼: $ENV_FILE"
    log_info "ëª¨ë‹ˆí„°ë§: $MONITORING_ENABLED"
}

# ì‚¬ì „ ì¡°ê±´ í™•ì¸
check_prerequisites() {
    log_step "ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì¤‘..."

    # Docker í™•ì¸
    if !# Day 5: ì»¨í…Œì´ë„ˆí™” ë° CI/CD (ìµœì‹  LangServe í†µí•©)     
```

## ì‚°ì¶œë¬¼ 5: ìµœì‹  ëª¨ë‹ˆí„°ë§ ì„¤ì •

### monitoring/prometheus.yml (LangServe ë©”íŠ¸ë¦­ í¬í•¨)
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'smart-learning'
    environment: '${ENVIRONMENT:-production}'

rule_files:
  - "alert_rules.yml"
  - "langchain_rules.yml"

scrape_configs:
  # Backend ì„œë¹„ìŠ¤ (Spring Boot Actuator)
  - job_name: 'smart-learning-backend'
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['backend:8080']
    scrape_interval: 30s
    scrape_timeout: 10s

  # AI ì„œë¹„ìŠ¤ (LangServe + Custom Metrics)
  - job_name: 'smart-learning-ai'
    static_configs:
      - targets: ['ai-service:8000']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 15s

  # LangChain ì¶”ì  ë©”íŠ¸ë¦­
  - job_name: 'langchain-metrics'
    static_configs:
      - targets: ['ai-service:8001']  # ë³„ë„ ë©”íŠ¸ë¦­ í¬íŠ¸
    metrics_path: '/langchain/metrics'
    scrape_interval: 60s

  # Frontend ì„œë¹„ìŠ¤
  - job_name: 'smart-learning-frontend'
    static_configs:
      - targets: ['frontend:80']
    metrics_path: '/metrics'
    scrape_interval: 60s

  # ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­
  - job_name: 'mysql-exporter'
    static_configs:
      - targets: ['mysql-exporter:9104']
    scrape_interval: 30s

  # Redis ë©”íŠ¸ë¦­
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 30s

  # Node ë©”íŠ¸ë¦­
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 30s

  # Docker ë©”íŠ¸ë¦­
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 30s

  # Nginx ë©”íŠ¸ë¦­
  - job_name: 'nginx-exporter'
    static_configs:
      - targets: ['nginx-exporter:9113']
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - 'alertmanager:9093'

# ìŠ¤í† ë¦¬ì§€ ì„¤ì •
storage:
  tsdb:
    retention.time: 30d
    retention.size: 10GB
```

### monitoring/langchain_rules.yml (LangChain ì „ìš© ì•Œë¦¼)
```yaml
groups:
  - name: langchain-alerts
    rules:
      # AI ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„
      - alert: LangChainHighLatency
        expr: histogram_quantile(0.95, rate(langchain_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "LangChain ì‘ë‹µ ì‹œê°„ì´ ë†’ìŠµë‹ˆë‹¤"
          description: "95th percentile ì‘ë‹µ ì‹œê°„ì´ {{ $value }}ì´ˆì…ë‹ˆë‹¤"

      # AI ëª¨ë¸ ì‹ ë¢°ë„
      - alert: LowAIConfidence
        expr: avg_over_time(langchain_response_confidence[15m]) < 0.5
        for: 10m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "AI ì‘ë‹µ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤"
          description: "í‰ê·  ì‹ ë¢°ë„ê°€ {{ $value | humanizePercentage }}ì…ë‹ˆë‹¤"

      # ë²¡í„° ìŠ¤í† ì–´ í¬ê¸°
      - alert: VectorStoreGrowth
        expr: increase(langchain_vector_store_documents_total[1h]) > 1000
        for: 0s
        labels:
          severity: info
          service: ai-service
        annotations:
          summary: "ë²¡í„° ìŠ¤í† ì–´ ë¹ ë¥¸ ì¦ê°€"
          description: "ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ {{ $value }}ê°œ ë¬¸ì„œ ì¶”ê°€ë¨"

      # OpenAI API ì˜¤ë¥˜ìœ¨
      - alert: OpenAIAPIErrors
        expr: rate(langchain_openai_api_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          service: ai-service
        annotations:
          summary: "OpenAI API ì˜¤ë¥˜ìœ¨ ë†’ìŒ"
          description: "OpenAI API ì˜¤ë¥˜ìœ¨ì´ {{ $value | humanizePercentage }}ì…ë‹ˆë‹¤"

      # í† í° ì‚¬ìš©ëŸ‰ ê¸‰ì¦
      - alert: HighTokenUsage
        expr: rate(langchain_token_usage_total[5m]) > 10000
        for: 5m
        labels:
          severity: warning
          service: ai-service
        annotations:
          summary: "í† í° ì‚¬ìš©ëŸ‰ ê¸‰ì¦"
          description: "ë¶„ë‹¹ {{ $value }}ê°œ í† í° ì‚¬ìš© ì¤‘"

  - name: smart-learning-business
    rules:
      # ì‚¬ìš©ì í™œë™
      - alert: LowUserActivity
        expr: rate(smart_learning_user_sessions_total[1h]) < 5
        for: 30m
        labels:
          severity: warning
          service: backend
        annotations:
          summary: "ì‚¬ìš©ì í™œë™ ê°ì†Œ"
          description: "ì‹œê°„ë‹¹ {{ $value }}ê°œ ì„¸ì…˜ë§Œ í™œì„±í™”ë¨"

      # í•™ìŠµ ì™„ë£Œìœ¨
      - alert: LowCompletionRate
        expr: (rate(smart_learning_course_completions_total[24h]) / rate(smart_learning_course_starts_total[24h])) < 0.3
        for: 2h
        labels:
          severity: warning
          service: backend
        annotations:
          summary: "í•™ìŠµ ì™„ë£Œìœ¨ ì €í•˜"
          description: "24ì‹œê°„ ì™„ë£Œìœ¨ì´ {{ $value | humanizePercentage }}ì…ë‹ˆë‹¤"
```

### monitoring/grafana/dashboards/smart-learning-overview.json
```json
{
  "dashboard": {
    "id": null,
    "title": "Smart Learning Overview",
    "tags": ["smart-learning", "overview"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Service Health",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=~\"smart-learning-.*\"}",
            "legendFormat": "{{job}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "AI Response Time (95th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(langchain_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "User Activity",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(smart_learning_user_sessions_total[5m]) * 60",
            "legendFormat": "Sessions per minute"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "AI Confidence Score",
        "type": "stat",
        "targets": [
          {
            "expr": "avg_over_time(langchain_response_confidence[5m])",
            "legendFormat": "Average Confidence"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.5},
                {"color": "green", "value": 0.8}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```
    
## ì‚°ì¶œë¬¼ 6: ìµœì‹  í™˜ê²½ ì„¤ì •

### .env.production (LangServe í†µí•©)
```env
# ì• í”Œë¦¬ì¼€ì´ì…˜ í™˜ê²½
ENVIRONMENT=production
COMPOSE_PROJECT_NAME=smart-learning
NODE_ENV=production

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DB_NAME=smart_learning_prod
DB_USER=prod_user
DB_PASSWORD=SuperSecurePassword123!@#
DB_ROOT_PASSWORD=RootSecurePassword456!@#
DB_PORT=3306

# Redis ì„¤ì •
REDIS_PASSWORD=RedisSecurePassword789!@#
REDIS_PORT=6379

# ë³´ì•ˆ ì„¤ì •
JWT_SECRET=ProductionJWTSecretKey_MustBe256BitsLong_123456789012345678901234567890

# AI ì„œë¹„ìŠ¤ ì„¤ì • (LangServe)
OPENAI_API_KEY=sk-your-openai-api-key-here
LANGCHAIN_API_KEY=your-langchain-api-key-here
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=smart-learning-production
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# AI ëª¨ë¸ ì„¤ì •
AI_MODEL_NAME=gpt-3.5-turbo
EMBEDDINGS_MODEL=text-embedding-3-small
AI_WORKERS=2
AI_MAX_CONCURRENT=10
AI_LOG_LEVEL=INFO

# ì„œë¹„ìŠ¤ í¬íŠ¸
BACKEND_PORT=8080
FRONTEND_PORT=80
AI_PORT=8000
REDIS_PORT=6379
DB_PORT=3306

# React í™˜ê²½ë³€ìˆ˜
REACT_APP_API_URL=https://api.smartlearning.com/api
REACT_APP_AI_URL=https://ai.smartlearning.com

# Spring Boot ì„¤ì •
SPRING_PROFILES_ACTIVE=production
MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE=health,info,metrics,prometheus
MANAGEMENT_ENDPOINT_HEALTH_SHOW_DETAILS=when_authorized

# ëª¨ë‹ˆí„°ë§
PROMETHEUS_PORT=9090
GRAFANA_PORT=3001
GRAFANA_PASSWORD=SecureGrafanaPassword!@#

# ì•Œë¦¼ ì„¤ì •
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
NOTIFICATION_EMAIL=admin@smartlearning.com

# ë°±ì—… ì„¤ì •
BACKUP_SCHEDULE=0 2 * * *
BACKUP_RETENTION_DAYS=30
BACKUP_S3_BUCKET=smart-learning-backups
AWS_REGION=ap-northeast-2

# ì„±ëŠ¥ íŠœë‹
JAVA_OPTS=-Xmx2g -Xms1g -XX:+UseG1GC -XX:+UseContainerSupport
PYTHON_WORKERS=2
NGINX_WORKER_PROCESSES=auto
```

### .env.development (ê°œë°œí™˜ê²½)
```env
# ê°œë°œ í™˜ê²½ ì„¤ì •
ENVIRONMENT=development
COMPOSE_PROJECT_NAME=smart-learning-dev
NODE_ENV=development

# ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DB_NAME=smart_learning_dev
DB_USER=dev_user
DB_PASSWORD=dev_password
DB_ROOT_PASSWORD=dev_root_password
DB_PORT=3307

# Redis ì„¤ì •
REDIS_PASSWORD=dev_redis_password
REDIS_PORT=6380

# ê°œë°œìš© ë³´ì•ˆ ì„¤ì •
JWT_SECRET=dev_jwt_secret_key_for_development_only

# AI ì„œë¹„ìŠ¤ ì„¤ì •
OPENAI_API_KEY=sk-your-dev-openai-api-key
LANGCHAIN_API_KEY=your-dev-langchain-api-key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=smart-learning-development

# AI ëª¨ë¸ ì„¤ì • (ê°œë°œìš©)
AI_MODEL_NAME=gpt-3.5-turbo
EMBEDDINGS_MODEL=text-embedding-3-small
AI_WORKERS=1
AI_MAX_CONCURRENT=5
AI_LOG_LEVEL=DEBUG

# ê°œë°œ ì„œë²„ í¬íŠ¸
BACKEND_PORT=8081
FRONTEND_PORT=3000
AI_PORT=8001

# React ê°œë°œ ì„¤ì •
REACT_APP_API_URL=http://localhost:8081/api
REACT_APP_AI_URL=http://localhost:8001
CHOKIDAR_USEPOLLING=true
FAST_REFRESH=true

# Spring Boot ê°œë°œ ì„¤ì •
SPRING_PROFILES_ACTIVE=dev
SPRING_DEVTOOLS_RESTART_ENABLED=true
SPRING_DEVTOOLS_LIVERELOAD_ENABLED=true

# ê°œë°œ ë„êµ¬
PROMETHEUS_PORT=9091
GRAFANA_PORT=3002
GRAFANA_PASSWORD=admin

# ë””ë²„ê¹…
DEBUG=true
LOG_LEVEL=DEBUG
```